{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Inspection\n",
    "\n",
    "About **ISWD**, & **oposum**\n",
    "\n",
    "Using the scratch/notebook to make code and data in **official repository** more clear and transparent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T18:04:10.384910Z",
     "start_time": "2021-07-15T18:04:08.177200Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T18:04:10.394905Z",
     "start_time": "2021-07-15T18:04:10.385910Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Project\\group-1.3\\ISWD\\iswd\n"
     ]
    }
   ],
   "source": [
    "%cd iswd/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T18:04:11.256422Z",
     "start_time": "2021-07-15T18:04:10.396905Z"
    }
   },
   "outputs": [],
   "source": [
    "from DataHandler import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seed word weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T18:20:20.291814Z",
     "start_time": "2021-07-15T18:20:20.287816Z"
    }
   },
   "outputs": [],
   "source": [
    "domain = 'bags_and_cases'\n",
    "data_folder = f'../data/oposum/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T18:20:21.651108Z",
     "start_time": "2021-07-15T18:20:21.601135Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(data_folder + f'preprocessed/{domain.upper()}_word_mapping.txt', 'r') as f:\n",
    "    word2id = {}\n",
    "    id2word = {}\n",
    "    for line in f:\n",
    "        word, id = line.strip().split()\n",
    "        id2word[int(id)] = word\n",
    "        word2id[word] = int(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T18:42:12.604095Z",
     "start_time": "2021-07-15T18:42:12.597099Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word 3080us not in vocab\n"
     ]
    }
   ],
   "source": [
    "with open(data_folder + f'seed_words/{domain}.30-weights.txt', 'r') as f:\n",
    "    aspects_ids = []\n",
    "    seed_weights = []\n",
    "    for line in f:\n",
    "        seeds = []\n",
    "        weights = []\n",
    "        for tok in line.split():\n",
    "            word, weight = tok.split(':')\n",
    "            if word in word2id:\n",
    "                seeds.append(word2id[word])\n",
    "                weights.append(float(weight))\n",
    "            else:\n",
    "                print(f\"word {word} not in vocab\")\n",
    "        aspects_ids.append(seeds)\n",
    "        seed_weights.append(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T18:46:36.659495Z",
     "start_time": "2021-07-15T18:46:36.654497Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T18:49:47.058049Z",
     "start_time": "2021-07-15T18:49:47.046054Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'pocket': 1,\n",
       "         'cable': 2,\n",
       "         'compartment': 1,\n",
       "         'outside': 1,\n",
       "         'lot': 2,\n",
       "         'wallet': 1,\n",
       "         'wish': 1,\n",
       "         'connector': 1,\n",
       "         'space': 2,\n",
       "         'flap': 1,\n",
       "         'charger': 1,\n",
       "         'folder': 2,\n",
       "         'pen': 1,\n",
       "         'power': 1,\n",
       "         'mouse': 1,\n",
       "         'thing': 2,\n",
       "         'inside': 2,\n",
       "         'nice': 1,\n",
       "         'small': 1,\n",
       "         'daytimer': 2,\n",
       "         'deforming': 1,\n",
       "         'noise': 2,\n",
       "         'headset': 1,\n",
       "         'control': 2,\n",
       "         'needing': 1,\n",
       "         'earphone': 1,\n",
       "         'huge': 1,\n",
       "         'instead': 1,\n",
       "         'accessory': 1,\n",
       "         'outer': 1,\n",
       "         'hassle': 2,\n",
       "         'return': 2,\n",
       "         'promptly': 2,\n",
       "         'free': 2,\n",
       "         'unable': 2,\n",
       "         'difficult': 2,\n",
       "         'arrived': 2,\n",
       "         'christmas': 2,\n",
       "         'replaced': 2,\n",
       "         'easy': 2,\n",
       "         'gift': 2,\n",
       "         'returned': 2,\n",
       "         'getting': 1,\n",
       "         'amazon': 2,\n",
       "         'week': 1,\n",
       "         'problem': 1,\n",
       "         'item': 2,\n",
       "         'case': 2,\n",
       "         'strap': 2,\n",
       "         'handle': 2,\n",
       "         'shoulder': 1,\n",
       "         'broke': 2,\n",
       "         'later': 2,\n",
       "         'month': 2,\n",
       "         'comfortable': 1,\n",
       "         'hand': 1,\n",
       "         'tear': 2,\n",
       "         'plastic': 2,\n",
       "         'wear': 2,\n",
       "         'ripped': 2,\n",
       "         'started': 2,\n",
       "         'finally': 2,\n",
       "         'swivel': 1,\n",
       "         'minute': 1,\n",
       "         'adjust': 1,\n",
       "         'loop': 1,\n",
       "         'adjustable': 1,\n",
       "         'hooked': 1,\n",
       "         'hurt': 1,\n",
       "         'bend': 1,\n",
       "         'attachment': 1,\n",
       "         'short': 1,\n",
       "         'torn': 1,\n",
       "         'holiday': 1,\n",
       "         'sooner': 1,\n",
       "         'break': 1,\n",
       "         'inevitable': 1,\n",
       "         'look': 1,\n",
       "         'color': 1,\n",
       "         'pink': 1,\n",
       "         'looked': 1,\n",
       "         'stylish': 1,\n",
       "         'pretty': 1,\n",
       "         'lime': 1,\n",
       "         'green': 1,\n",
       "         'fashionable': 1,\n",
       "         'awesome': 1,\n",
       "         'picture': 1,\n",
       "         'good': 3,\n",
       "         'great': 2,\n",
       "         'design': 1,\n",
       "         'sleek': 1,\n",
       "         'cell': 2,\n",
       "         'rubbery': 1,\n",
       "         'beautiful': 1,\n",
       "         'sacrifice': 1,\n",
       "         'cute': 1,\n",
       "         'style': 1,\n",
       "         'durability': 1,\n",
       "         'wanted': 1,\n",
       "         'needed': 1,\n",
       "         'wich': 1,\n",
       "         'lil': 1,\n",
       "         'quilted': 1,\n",
       "         'professional': 1,\n",
       "         'black': 1,\n",
       "         'pattern': 1,\n",
       "         'recommend': 1,\n",
       "         'bought': 1,\n",
       "         'used': 1,\n",
       "         'hard': 1,\n",
       "         'bag': 1,\n",
       "         'work': 1,\n",
       "         'using': 1,\n",
       "         'pleased': 1,\n",
       "         'backpack': 1,\n",
       "         'weight': 1,\n",
       "         'say': 1,\n",
       "         'new': 1,\n",
       "         'purchased': 1,\n",
       "         'carrying': 1,\n",
       "         'stop': 1,\n",
       "         'decided': 1,\n",
       "         'saw': 1,\n",
       "         'logic': 1,\n",
       "         'drawback': 1,\n",
       "         'thought': 1,\n",
       "         'complaint': 1,\n",
       "         'true': 1,\n",
       "         'review': 1,\n",
       "         'laptop': 3,\n",
       "         'plane': 1,\n",
       "         'description': 1,\n",
       "         'ultrabook': 1,\n",
       "         'highly': 1,\n",
       "         'tell': 1,\n",
       "         'price': 1,\n",
       "         'worth': 1,\n",
       "         'pay': 1,\n",
       "         '100': 1,\n",
       "         'buck': 1,\n",
       "         'reasonable': 1,\n",
       "         'inexpensive': 1,\n",
       "         'seen': 1,\n",
       "         'based': 1,\n",
       "         'beleive': 1,\n",
       "         'suppose': 1,\n",
       "         'overly': 1,\n",
       "         'requested': 1,\n",
       "         'extra': 1,\n",
       "         'compare': 1,\n",
       "         'store': 1,\n",
       "         'leather': 1,\n",
       "         'close': 1,\n",
       "         'expensive': 1,\n",
       "         'money': 1,\n",
       "         'doesnt': 1,\n",
       "         'product': 1,\n",
       "         'given': 1,\n",
       "         '10': 1,\n",
       "         'definitely': 1,\n",
       "         'cheap': 1,\n",
       "         'overall': 1,\n",
       "         'protection': 1,\n",
       "         'padding': 1,\n",
       "         'protects': 1,\n",
       "         'drop': 1,\n",
       "         'protect': 1,\n",
       "         'lap': 1,\n",
       "         'computer': 1,\n",
       "         'protective': 1,\n",
       "         'cushioning': 1,\n",
       "         'let': 1,\n",
       "         'secure': 1,\n",
       "         'sort': 1,\n",
       "         'adequate': 1,\n",
       "         'happen': 1,\n",
       "         'internal': 1,\n",
       "         'weak': 1,\n",
       "         'trust': 1,\n",
       "         'second': 1,\n",
       "         'damaged': 1,\n",
       "         'protected': 1,\n",
       "         'provides': 1,\n",
       "         'count': 1,\n",
       "         'taste': 1,\n",
       "         'constructed': 1,\n",
       "         'large': 1,\n",
       "         'pencil': 1,\n",
       "         'quality': 1,\n",
       "         'material': 1,\n",
       "         'poor': 1,\n",
       "         'durable': 1,\n",
       "         'chemical': 1,\n",
       "         'zipper': 1,\n",
       "         'excellent': 1,\n",
       "         'terrible': 1,\n",
       "         'crack': 1,\n",
       "         'white': 1,\n",
       "         'edge': 1,\n",
       "         'pull': 1,\n",
       "         'smell': 1,\n",
       "         'daily': 1,\n",
       "         'year': 1,\n",
       "         'feel': 1,\n",
       "         'going': 1,\n",
       "         'messy': 1,\n",
       "         'fit': 1,\n",
       "         'size': 1,\n",
       "         'macbook': 1,\n",
       "         'big': 1,\n",
       "         'air': 1,\n",
       "         'slightly': 1,\n",
       "         'bulk': 1,\n",
       "         'perfect': 1,\n",
       "         'inch': 1,\n",
       "         'perfectly': 1,\n",
       "         '13': 1,\n",
       "         'snug': 1,\n",
       "         'notebook': 1,\n",
       "         'sure': 1,\n",
       "         'bit': 1,\n",
       "         'te': 1,\n",
       "         'jacob': 1,\n",
       "         'clothes': 1,\n",
       "         'port': 1,\n",
       "         'larger': 1,\n",
       "         'sealing': 1})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([id2word[a] for asp in aspects_ids for a in asp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sum for each aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T18:42:18.101322Z",
     "start_time": "2021-07-15T18:42:18.096324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4850099999999997\n",
      "9.45038\n",
      "1.9780100000000005\n",
      "1.4594199999999993\n",
      "0.20390000000000005\n",
      "3.885949999999999\n",
      "2.571000000000001\n",
      "0.9063899999999999\n",
      "0.8045999999999998\n"
     ]
    }
   ],
   "source": [
    "for w in seed_weights:\n",
    "    print(np.sum(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data from official code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataHandler import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DataHandler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-13T15:14:44.836295Z",
     "start_time": "2021-07-13T15:14:44.767336Z"
    }
   },
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-13T15:14:45.532883Z",
     "start_time": "2021-07-13T15:14:45.530881Z"
    }
   },
   "outputs": [],
   "source": [
    "domain = 'bags_and_cases'\n",
    "data_path = f'./data/oposum/preprocessed/{domain.upper()}.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-13T15:14:52.972592Z",
     "start_time": "2021-07-13T15:14:46.205494Z"
    }
   },
   "outputs": [],
   "source": [
    "offi_train = []\n",
    "offi_idx = []\n",
    "f = h5py.File(data_path, 'r')\n",
    "for b in f['data']:\n",
    "    offi_train.extend(list(f['original/' + b][()]))\n",
    "    offi_idx.extend(list(f['data/' + b][()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-13T15:15:08.718557Z",
     "start_time": "2021-07-13T15:15:08.567626Z"
    }
   },
   "outputs": [],
   "source": [
    "# offi_train\n",
    "offi_train_str = list(map(lambda x: x.decode(), offi_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-13T15:15:09.537068Z",
     "start_time": "2021-07-13T15:15:09.532070Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "588229\n",
      "588229\n"
     ]
    }
   ],
   "source": [
    "print(len(offi_train))\n",
    "print(len(offi_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-13T15:15:16.250268Z",
     "start_time": "2021-07-13T15:15:16.243271Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eg_str = \"you 've\"\n",
    "eg_str = \"when you 're\"\n",
    "eg_idx = offi_train_str.index(eg_str)\n",
    "offi_idx[eg_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-13T15:10:03.037466Z",
     "start_time": "2021-07-13T15:10:03.030469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aren', 'not', 'a', 'did', 'it', 't', 'yourselves', 'themselves', 'should', 'they', 'just', 'doesn', \"don't\", 'these', \"you'd\", 'mightn', \"you're\", 'the', 'into', 'that', 'own', 'his', 'again', 'how', 'haven', 'hasn', 've', 'who', 'after', \"wouldn't\", 'now', 'what', 'out', 'before', 'further', \"needn't\", 'shouldn', 'whom', 'ain', \"couldn't\", 'during', 'than', 'is', 'up', 'there', 'until', \"doesn't\", 'her', 'them', 'you', 'was', 'can', 'this', 'o', \"mustn't\", 'to', 'over', 'most', 'wouldn', 'd', 'above', \"didn't\", 'me', 'having', 'no', 'an', 'where', 'theirs', 'your', 'my', 'on', 'being', 'only', \"should've\", 'were', 'will', 'she', 'be', 'each', 'some', 'very', 'hadn', 'while', 'himself', 'does', 'from', 'herself', 'why', 'he', 'by', 'have', 'all', 'more', 're', 'doing', 'isn', \"she's\", 'am', 'in', 'between', 'him', \"weren't\", \"that'll\", 'then', 'myself', 'through', 'both', 'are', 'too', 'as', 'nor', 'here', \"mightn't\", 'against', \"aren't\", 'hers', 'under', 'shan', 'or', 'won', 'those', \"you've\", \"wasn't\", 'because', 'mustn', 'ours', 'once', 'had', 'but', 'our', 'about', 'off', 'so', 'll', 'yourself', 'below', \"hasn't\", 'such', 'which', 'same', 'we', 'of', 'few', \"hadn't\", 'its', \"haven't\", 'ourselves', 'didn', 'with', 'when', \"shan't\", 'at', 'wasn', 'weren', 'ma', 'couldn', 'needn', 'has', 'for', 'their', \"won't\", 'any', \"you'll\", 'itself', \"it's\", 'yours', 's', \"shouldn't\", 'if', 'm', \"isn't\", 'do', 'and', 'other', 'been', 'don', 'y', 'i', 'down'}\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stw_list = set(stopwords.words('english'))\n",
    "print(stw_list)\n",
    "print(\"'ve\" in stw_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(offi_idx, key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-13T15:58:35.837689Z",
     "start_time": "2021-07-13T15:58:35.437903Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'\", 'e', 'C', '_', \"'\", '1', 'x', 'x', \"'\", '1']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sorted(offi_train_str, key=len)\n",
    "sorted(offi_train_str, key=len)[:10]\n",
    "# sorted(offi_train_str, key=len)[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data from unofficial code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = 'bags_and_cases'\n",
    "data_path = f'../LeverageJustAFewKeywords/data/{domain}_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path, 'r') as f:\n",
    "    json_file = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "588229\n",
      "11765\n"
     ]
    }
   ],
   "source": [
    "# unoffi_train['original']\n",
    "print(np.sum([len(batch) for batch in json_file['original']]))   # total number of segments\n",
    "print(len(json_file['original']))    # number of batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "unoffi_train = [s for b in json_file['original'] for s in b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['that opens easily on top with a zipper at the TSA line , iPad , chargers , adapters , extra pens , passport , travel cards , meds , gum , wallet full of loyalty cards , folder of receipts , cough drops , tissues , other necessities , etc. under the flap in the roomy main area and in assorted pockets , and business cards , id badges , head set , memory sticks , sunglasses , and loose change on the outside pockets .',\n",
       " 'that looks more for a passport than a card . a huge pocket for papers , and a hidden pocket for more papers or maybe a notebook , agenda , whatever . and of course , a pen holder for your pen . not a single pocket for memory cards like SDHC or CF , or maybe a pocket for an external CD/DVD-RW like my Samsung SE-S084 . anyway even with the pocket , will simply not fit for the lack of space so forget fill the card/papers slots',\n",
       " \"I 'm able to carry a set of tools , a 50-pack spindle of CDR 's and DVDR 's , an array of hard-plastic double-CD/DVD cases containing various work-related images and utilities , a 3 . 5 '' hard drive w/shockproof packaging , a few CAT6 and USB cables , AC adaptors for a Lenovo and MacBook Pro , several USB thumbdrives , a handheld label printer , a stash of low-carb energy bars , a few writing utensils , and the occasional book .\",\n",
       " 'I was able to stow a far greater amount of items inside than anticipated : an ASUS K50 15 . 6-inch laptop with charger , cord and spare battery ; a small 500 GB USB backup drive ; a Kindle Fire and a Kindle 2 ; a BlackBerry plus its charger ; a compact digital camera with a spare battery ; a small notebook ; pens and pencils ; a small umbrella ; a supply of business cards ; two books and a catalog ; misc . cables and a couple of flash drives ; etc. .',\n",
       " 'Adult Head & Purple Tubing , A Whitecoat Clipboard , a wallet , keys , work badge , Ipad , small brush , deodorant , 3 small bottles of lotion , 3 highlighters , 3 pens , 3 packages of gum , a package ofColgate Wisp Mini-brushes Spearmint - 4 in a Pk , Sold As 1 Pk , a bottle Excedrin , a Motorola Android cell phone , aNative Union Moshi Moshi Retro POP Handset for iPhone , iPad , iPod , and Android Phones - Soft Touch - Pink , and a small cream perfume and eyeliner .']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(unoffi_train, key=len)[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT embedding\n",
    "\n",
    "statistics: in `bags_and_cases` training files, there are 588229 segments -> found that the code just take pre-computed BERT embedding from the data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = \"bags_and_cases\"\n",
    "method = \"train\"\n",
    "bert_folder = \"../data/pretrained_bert/oposum/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_scodes = joblib.load(bert_folder + \"{}_{}_scodes.pkl\".format(domain, method))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "588229"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bert_scodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = \"../data/oposum/preprocessed/{}.hdf5\".format(domain.upper())\n",
    "train_file = h5py.File(train_file_path, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "588229"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_origial = []\n",
    "for b in train_file['original']:\n",
    "    train_origial.extend(train_file['original/' + b][()])\n",
    "train_file.close()\n",
    "len(train_origial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec\n",
    "\n",
    "vocab length for bags_and_cases -> 30423\n",
    "\n",
    "we use train/dev/test for building vocab, so the vocab size would be little larger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_domains = ['bags_and_cases', 'bluetooth', 'boots', 'keyboards', 'tv', 'vacuums']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "domain bags_and_cases vocab length: 30423\n",
      "\n",
      "domain bluetooth vocab length: 51229\n",
      "\n",
      "domain boots vocab length: 30332\n",
      "\n",
      "domain keyboards vocab length: 34066\n",
      "\n",
      "domain tv vocab length: 59017\n",
      "\n",
      "domain vacuums vocab length: 46228\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for domain in all_domains:\n",
    "    data_file = './data/oposum/preprocessed/' + domain.upper() + '.hdf5'\n",
    "    f = h5py.File(data_file, 'r')\n",
    "    wv = f['w2v'][()]\n",
    "    print(f\"domain {domain} vocab length: {len(wv)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# draft / scratch paper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list([1, 2, 3, 4]) list([2, 3]) list([4, 5])]\n",
      "[list([1, 2, 3, 4]) list([2, 3]) list([4, 5])]\n",
      "<class 'list'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ipykernel_launcher:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    }
   ],
   "source": [
    "a = [[1, 2, 3, 4], [2, 3], [4, 5]]\n",
    "b = np.array(a)\n",
    "print(b)\n",
    "c = np.array(a, dtype=object)\n",
    "print(c)\n",
    "print(type(c[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['string1' 'a longer string' 'a much longer string']\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "a = ['string1', 'a longer string', 'a much longer string']\n",
    "b = np.array(a)\n",
    "print(b)\n",
    "print(type(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(12)\n",
    "b = a[3:4]\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d7be162faa9619c70b8a448037d12fbee0f966914265961e1eb040c7257eee03"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
