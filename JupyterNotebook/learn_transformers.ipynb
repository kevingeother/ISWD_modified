{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Transformers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from transformers import AutoTokenizer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\r\n",
    "encoded_input = tokenizer(\"Hello, I'm a single sentence!\")\r\n",
    "print(encoded_input)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'input_ids': [101, 8667, 117, 146, 112, 182, 170, 1423, 5650, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "tokenizer.decode(encoded_input[\"input_ids\"])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"[CLS] Hello, I'm a single sentence! [SEP]\""
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "tokenizer.encode(\"Hello, I'm a single sentence!\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[101, 8667, 117, 146, 112, 182, 170, 1423, 5650, 106, 102]"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# encoded_input_token = tokenizer([\"Hello\", \"I'm\", \"a\", \"single\", \"sentence\"], is_split_into_words=True)\r\n",
    "encoded_input_token = tokenizer([\"Hello\", \",\", \"I'm\", \"a\", \"single\", \"sentence\", \"!\"], is_split_into_words=True)\r\n",
    "print(encoded_input_token)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'input_ids': [101, 8667, 117, 146, 112, 182, 170, 1423, 5650, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "tokenizer.decode(encoded_input_token['input_ids'])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"[CLS] Hello I'm a single sentence [SEP]\""
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "batch_text = [\"That was another administration.\",\r\n",
    "        \"Why is the ex CEO of Monsanto appointed head of the FDA ?\",\r\n",
    "        \"because he greased the palms of our \\\"elected\\\" leaders.\",\r\n",
    "        \"and judges too\",\r\n",
    "        \"the photo shown above is from a feed lot in California.it is labeled as such in the article.the claims are that the environmental impact of grass fed cows and feed lot cows are the same.which of course is not.just like you said- animals that graze on pastures continue the growth/life cycle of the forage if managed well\",\r\n",
    "        \"THAT IS NOT GRASS FED MEET!\",\r\n",
    "        \"NO GRASS THERE!\",\r\n",
    "        \"YOU ARE FAKE NEWS!\",\r\n",
    "        \"IF ANIMALS COULD RUN THE RANGE LIKE THEY USED TO, THE POOP FERTILIZES THE GRASS AND THE GRASS GROWS AND THE RAIN COMES AND WE ARE ALL HAPPY!\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# batch_ids = tokenizer(batch_text, padding=True)['input_ids']\r\n",
    "batch_ids = tokenizer(batch_text)['input_ids']\r\n",
    "# tokenizer(batch_text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# batch_ids"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "'bert-case-uncased'.startswith('bert')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Summary of the tokenizers\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "nltk_tokenizer = RegexpTokenizer(r'\\w+')\r\n",
    "sentence = \"Don't you love Transformers? We sure do.\"\r\n",
    "nltk_token = nltk_tokenizer.tokenize(sentence)\r\n",
    "print(nltk_token)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Don', 't', 'you', 'love', 'Transformers', 'We', 'sure', 'do']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# fine-tuning \r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from datasets import load_dataset"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d7be162faa9619c70b8a448037d12fbee0f966914265961e1eb040c7257eee03"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('nlp': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}