{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "oposum_glove_finetune.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "gBZgg-aqSrFv",
        "1NeDBKU12EoH",
        "J7Rj9DF8THin",
        "E4TVwstvtVY8",
        "ToT1Be8IUZjK"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBZgg-aqSrFv"
      },
      "source": [
        "#Setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwyU1zYewGSq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "768f7d56-ef65-40af-de6d-b63bbc9ce190"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# %cd /content/drive/MyDrive/group-1.3-master/group-1.3-master/LeverageJustAFewKeywords/\n",
        "# %cd /content/drive/MyDrive/LeverageJustAFewKeywords/\n",
        "%cd /content/drive/MyDrive/group-1.3/LeverageJustAFewKeywords/\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/.shortcut-targets-by-id/1n0oSoMBR4TlxDwAce51xBgon3LxJjCkE/group-1.3/LeverageJustAFewKeywords\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1hYIprkDdZJ",
        "outputId": "14f4bede-01a8-4c0f-df6f-6f2e0b86c9f2"
      },
      "source": [
        "!pip install mittens\n",
        "import csv\n",
        "import numpy as np\n",
        "from mittens import GloVe, Mittens\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import os\n",
        "from time import time\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "nltk.download('words')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mittens\n",
            "  Downloading https://files.pythonhosted.org/packages/ce/c0/6e4fce5b3cb88edde2e657bb4da9885c0aeac232981706beed7f43773b00/mittens-0.2-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mittens) (1.19.5)\n",
            "Installing collected packages: mittens\n",
            "Successfully installed mittens-0.2\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80qDErFH9gR_"
      },
      "source": [
        "#Pre Trained"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1S_GNJo9kKM"
      },
      "source": [
        "1.   Find words that are in corpus but not in pre-trained vocab. (called corp_vocab in this method)\n",
        "2.   create np.random for these words\n",
        "3.   Merge new embeddings with pre-trained embeddings\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k033x-DG9fUc"
      },
      "source": [
        "def glove2dict(glove_filename):\n",
        "    with open(glove_filename, encoding='utf-8') as f:\n",
        "        reader = csv.reader(f, delimiter=' ', quoting=csv.QUOTE_NONE)\n",
        "        embed = {line[0]: np.array(list(map(float, line[1:])))\n",
        "                for line in reader}\n",
        "    return embed\n",
        "\n",
        "glove_path = \"../glove.6B/glove.6B.300d.txt\" # get it from https://nlp.stanford.edu/projects/glove\n",
        "original_embeddings = glove2dict(glove_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BS7my7_j9734"
      },
      "source": [
        "def pickle_load(filename):\n",
        "    with open(filename, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "def pickle_save(clean_corpus, filename):\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(clean_corpus, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqcEDPEc-BeV"
      },
      "source": [
        "domain = 'bags_and_cases'\n",
        "corpus_file = '../processed/oposum/' + domain + '_corpus_wotf1.pkl'\n",
        "corpus = pickle_load(corpus_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTL5fCcp-BeX"
      },
      "source": [
        "vocab = set()\n",
        "for sentence in corpus:\n",
        "  for item in sentence:\n",
        "    vocab.add(item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHlQtP55-BeY",
        "outputId": "46a1c99f-9393-490a-f6bb-d6a669f22654"
      },
      "source": [
        "corp_vocab = [token for token in vocab if token not in original_embeddings.keys()] # out of glove-vocab words specific to corpus\n",
        "print(corp_vocab[:10])\n",
        "print(len(corp_vocab))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['manfrotto', '1080cc', 'jsut', 'tranporting', 'shoudler', 'biggy', 'overnighter', 'mbdedicated', 'shlepping', 'rollaway']\n",
            "2131\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rs5RJLirlOzy"
      },
      "source": [
        "random_embeds = np.random.rand(len(corp_vocab),300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtUiKL5umbM4"
      },
      "source": [
        "new_glove = dict(zip(corp_vocab, random_embeds))\n",
        "# extra_glove['theswissgear']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bV1TUKD-x5Q"
      },
      "source": [
        "old_glove = {item:original_embeddings[item] for item in vocab if item in original_embeddings}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38IYt5Qh-x5S"
      },
      "source": [
        "pretrained_glove = {**old_glove, **new_glove} #combining old and new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4Z71Pfp-x5U",
        "outputId": "c194d744-891b-4e9e-b0f3-b65e4ee7517a"
      },
      "source": [
        "len(pretrained_glove)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15429"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGC7JOrz-6FN"
      },
      "source": [
        " finetune_output_dir = '../wv/oposum_w2v/'\n",
        " pickle_save(pretrained_glove, finetune_output_dir + domain + '_glove_pretrained.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NeDBKU12EoH"
      },
      "source": [
        "#Fine tune for one domain: Method 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DS6-U4M8qs7j"
      },
      "source": [
        "1.   Find words that are in corpus but not in pre-trained vocab. (called corp_vocab in this method)\n",
        "2.   Create co-occurence matrix for corp_vocab\n",
        "3.   Train Glove on this co-occurence matrix\n",
        "4.   Merge new embeddings with pre-trained embeddings\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Mj0Q3Bl2VU2"
      },
      "source": [
        "def glove2dict(glove_filename):\n",
        "    with open(glove_filename, encoding='utf-8') as f:\n",
        "        reader = csv.reader(f, delimiter=' ', quoting=csv.QUOTE_NONE)\n",
        "        embed = {line[0]: np.array(list(map(float, line[1:])))\n",
        "                for line in reader}\n",
        "    return embed\n",
        "\n",
        "glove_path = \"../glove.6B/glove.6B.300d.txt\" # get it from https://nlp.stanford.edu/projects/glove\n",
        "original_embeddings = glove2dict(glove_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fk38Qokg2VU3"
      },
      "source": [
        "def pickle_load(filename):\n",
        "    with open(filename, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "def pickle_save(clean_corpus, filename):\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(clean_corpus, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcsCLXMJ2VU4"
      },
      "source": [
        "domain = 'bags_and_cases'\n",
        "corpus_file = '../processed/oposum/' + domain + '_corpus_wotf1.pkl'\n",
        "corpus = pickle_load(corpus_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWXDHWgt2VU_"
      },
      "source": [
        "vocab = set()\n",
        "count = defaultdict(int)\n",
        "for sentence in corpus:\n",
        "  for item in sentence:\n",
        "    vocab.add(item)\n",
        "    count[item] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwR9NKik2aHq",
        "outputId": "23e98230-dd3d-4bee-de5d-2223c63403e5"
      },
      "source": [
        "print(list(vocab)[:10])\n",
        "print(len(vocab))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['armour', 'boyscout', 'packrat', 'm6700', 'relative', 'lengthens', 'account', 'hostile', 'blend', 'mare']\n",
            "15429\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDwZ4iE02bd7",
        "outputId": "682e9826-e959-409d-b566-9ceb814e853d"
      },
      "source": [
        "corpus_doc = [' '.join(item) for item in corpus]\n",
        "print(corpus_doc[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "case look nice plenty pocket stuff carry around 're using something back forth office thinner laptop enough padding protect computer scratching rubbing bought son school past fall one thinner laptop cover soon dented scratched constant rubbing screen touchpad control left little white mark display blame computer design added padding case problem plus side case durable nothing yet ripped torn 's easy spot clean\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkqcsn5ECB3k",
        "outputId": "3b9b4a22-d817-49c7-80a8-577e995519aa"
      },
      "source": [
        "corp_vocab = [token for token in vocab if token not in original_embeddings.keys()] # out of glove-vocab words specific to corpus\n",
        "print(corp_vocab[:10])\n",
        "print(len(corp_vocab))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['boyscout', 'm6700', 'favs', 'overstuff', 'extreemly', 'dakine', 'backpain', '3n1', '2013i', 'biggy']\n",
            "2131\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5WL4Dpt3Xz3",
        "outputId": "606b1fe3-d6d6-4450-f93d-58d510eb2859"
      },
      "source": [
        "t0=time()\n",
        "count_model = CountVectorizer(ngram_range=(1,5),vocabulary=corp_vocab) # unigram to 5-gram. building only for corpus specific vocab\n",
        "X = count_model.fit_transform(corpus_doc)\n",
        "X[X > 0] = 1 # to remove within-line cooccurence\n",
        "Xc = (X.T * X) # co-occurrence matrix in sparse csr format\n",
        "Xc.setdiag(0) # setting same word cooccurence to 0\n",
        "# coocc_arr = sparse.lil_matrix(Xc).toarray()\n",
        "coocc_ar = Xc.toarray()\n",
        "t1= time()\n",
        "print(f\" cost {t1 - t0:.2f} seconds\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " cost 6.15 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csc_matrix is expensive. lil_matrix is more efficient.\n",
            "  self._set_arrayXarray(i, j, x)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "068-JhgJC2fz",
        "outputId": "0b9c1711-2f0d-4eec-d58f-94f2aad3bb01"
      },
      "source": [
        "t0=time()\n",
        "glove_model = GloVe(n=300, max_iter=500)\n",
        "new_embeddings = glove_model.fit(coocc_ar)\n",
        "t1= time()\n",
        "print(f\"\\ntraining cost {t1 - t0:.2f} seconds\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py:1761: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n",
            "Iteration 500: loss: 0.0013275939272716641"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training cost 34.44 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGJu3LXRGWvK"
      },
      "source": [
        "new_embeds = dict(zip(corp_vocab, new_embeddings))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Dzwdu6bDLqv"
      },
      "source": [
        "old_embeds = {item:original_embeddings[item] for item in vocab if item in original_embeddings}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOd2X6evFAsv"
      },
      "source": [
        "finetuned_embeddings = {**old_embeds, **new_embeds} #combining old and new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FSidTODGdf0",
        "outputId": "e9124d33-8a19-4e40-9934-8c928b54631c"
      },
      "source": [
        "len(finetuned_embeddings)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15429"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OwDyg8HGxyb"
      },
      "source": [
        " finetune_output_dir = '../wv/oposum_w2v/'\n",
        " pickle_save(finetuned_embeddings, finetune_output_dir + domain + '_glove_tuned_m1.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7Rj9DF8THin"
      },
      "source": [
        "# Fine tune for one domain: Method 2\n",
        "Warning: runs out of RAM in free Colab and notebook crashes after 20 seconds if size of vocab greater than 12.5k tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMwsD_tTsD9K"
      },
      "source": [
        "1.   Find words that are in corpus and in pre-trained vocab. (called vocab in this method)\n",
        "2.   Create co-occurence matrix for vocab\n",
        "3.   Use Mittens to fine tune on this co-occurence matrix\n",
        "4.   Save New embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElbfaIORUBJ0"
      },
      "source": [
        "def glove2dict(glove_filename):\n",
        "    with open(glove_filename, encoding='utf-8') as f:\n",
        "        reader = csv.reader(f, delimiter=' ', quoting=csv.QUOTE_NONE)\n",
        "        embed = {line[0]: np.array(list(map(float, line[1:])))\n",
        "                for line in reader}\n",
        "    return embed\n",
        "\n",
        "glove_path = \"../glove.6B/glove.6B.300d.txt\" # get it from https://nlp.stanford.edu/projects/glove\n",
        "original_embeddings = glove2dict(glove_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67ij096oci2l"
      },
      "source": [
        "def pickle_load(filename):\n",
        "    with open(filename, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "def pickle_save(clean_corpus, filename):\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(clean_corpus, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qZz5vT0UhGb"
      },
      "source": [
        "domain = 'bags_and_cases'\n",
        "corpus_file = '../processed/oposum/' + domain + '_corpus_wotf1.pkl'\n",
        "corpus = pickle_load(corpus_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5-EKAdRUjd5"
      },
      "source": [
        "vocab = set()\n",
        "for sentence in corpus:\n",
        "  for item in sentence:\n",
        "    vocab.add(item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmNQ0uwvs3pC",
        "outputId": "c2045fec-a295-4811-81c8-0d930d7b4d93"
      },
      "source": [
        "corpus_doc = [' '.join(item) for item in corpus]\n",
        "print(corpus_doc[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "case look nice plenty pocket stuff carry around 're using something back forth office thinner laptop enough padding protect computer scratching rubbing bought son school past fall one thinner laptop cover soon dented scratched constant rubbing screen touchpad control left little white mark display blame computer design added padding case problem plus side case durable nothing yet ripped torn 's easy spot clean\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQoc3OBh4AM1"
      },
      "source": [
        "Create co-occurence matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmnvAqame1uF",
        "outputId": "e236b3bc-6830-4f1a-af70-4f3a6458f909"
      },
      "source": [
        "t0=time()\n",
        "count_model = CountVectorizer(ngram_range=(1,5),vocabulary=vocab) # unigram to 5-gram.\n",
        "X = count_model.fit_transform(corpus_doc)\n",
        "X[X > 0] = 1 # to remove within-line cooccurence\n",
        "Xc = (X.T * X) # co-occurrence matrix in sparse csr format\n",
        "Xc.setdiag(0) # setting same word cooccurence to 0\n",
        "# coocc_arr = sparse.lil_matrix(Xc).toarray()\n",
        "coocc_ar = Xc.toarray()\n",
        "t1= time()\n",
        "print(f\" cost {t1 - t0:.2f} seconds\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csc_matrix is expensive. lil_matrix is more efficient.\n",
            "  self._set_arrayXarray(i, j, x)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " cost 9.41 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5oDZ-NAcs80"
      },
      "source": [
        "Mittens Training - crash here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1L68Xg6UjRJ",
        "outputId": "4f132c3b-aec1-446a-823b-744a91563c17"
      },
      "source": [
        "mittens_model = Mittens(n=300, max_iter=500)\n",
        "t0 = time()\n",
        "new_embeddings = mittens_model.fit(\n",
        "    coocc_ar,\n",
        "    vocab=vocab,\n",
        "    initial_embedding_dict= original_embeddings)\n",
        "t1=time()\n",
        "print(f\"\\ntraining cost {t1 - t0:.2f} seconds\")\n",
        "new_glove = dict(zip(vocab, new_embeddings))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py:1761: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0APH2KHtE5P"
      },
      "source": [
        "len(new_glove)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXJUB741tDPw"
      },
      "source": [
        "finetune_output_dir = '../wv/oposum_w2v/' \n",
        "pickle_save(new_glove, finetune_output_dir + domain + '_glove_tuned_m2.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-I2elCgcooq"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "Testing difference with Glove - also crashes\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dq-4BuP5Oa7",
        "outputId": "180d4583-b630-4f4e-e693-a451d44fb7b2"
      },
      "source": [
        "t0=time()\n",
        "glove_model = GloVe(n=300, max_iter=500)\n",
        "test_embeddings = glove_model.fit(coocc_ar)\n",
        "t1= time()\n",
        "print(f\"training cost {t1 - t0:.2f} seconds\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/adagrad.py:77: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4TVwstvtVY8"
      },
      "source": [
        "# Fine tune for one domain: Method 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qX2b9l6AtbCq"
      },
      "source": [
        "Reduce domain specific vocab size (under 12.5k) in order to fit in memory. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwMAAZn2tlVS"
      },
      "source": [
        "This could mean loss a lot of words. Bags and cases is still at 15k vocab after removing tf1 words. 20% reduction in size?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2viHA0z3Avtw"
      },
      "source": [
        "1.   Find words that are in corpus and in pre-trained vocab. (called vocab in this method)\n",
        "2.   Remove some more words from vocab with low occurence frequency\n",
        "3.   Create co-occurence matrix for vocab\n",
        "4.   Train Mittens on this co-occurence matrix\n",
        "\n",
        "\n",
        "5.   Find words that are in corpus but not in pre-trained vocab. (called corp_vocab in this method)\n",
        "2.   Create co-occurence matrix for corp_vocab\n",
        "3.   Train Glove on this co-occurence matrix\n",
        "\n",
        "\n",
        "4.   Merge fine-tuned embeddings with new embeddings\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlB-MtqGAQTc"
      },
      "source": [
        "def glove2dict(glove_filename):\n",
        "    with open(glove_filename, encoding='utf-8') as f:\n",
        "        reader = csv.reader(f, delimiter=' ', quoting=csv.QUOTE_NONE)\n",
        "        embed = {line[0]: np.array(list(map(float, line[1:])))\n",
        "                for line in reader}\n",
        "    return embed\n",
        "\n",
        "glove_path = \"../glove.6B/glove.6B.300d.txt\" # get it from https://nlp.stanford.edu/projects/glove\n",
        "original_embeddings = glove2dict(glove_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLPjsXRpAQT4"
      },
      "source": [
        "def pickle_load(filename):\n",
        "    with open(filename, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "def pickle_save(clean_corpus, filename):\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(clean_corpus, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swwzsZEbAQT5"
      },
      "source": [
        "domain = 'bags_and_cases'\n",
        "corpus_file = '../processed/oposum/' + domain + '_corpus_wotf1.pkl'\n",
        "corpus = pickle_load(corpus_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28DTQd7rAQT6"
      },
      "source": [
        "vocab = set()\n",
        "count = defaultdict(int)\n",
        "for sentence in corpus:\n",
        "  for item in sentence:\n",
        "    vocab.add(item)\n",
        "    count[item] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJZj0IGeAQT7",
        "outputId": "db7b8e09-cda7-4a79-ab77-64f65f048cdf"
      },
      "source": [
        "print(list(vocab)[:10])\n",
        "print(len(vocab))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['armour', 'boyscout', 'packrat', 'm6700', 'relative', 'lengthens', 'account', 'hostile', 'blend', 'mare']\n",
            "15429\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jF2MPKxnAQT9",
        "outputId": "9c09216e-b007-4392-cabe-9fa8cbf21167"
      },
      "source": [
        "corpus_doc = [' '.join(item) for item in corpus]\n",
        "print(corpus_doc[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "case look nice plenty pocket stuff carry around 're using something back forth office thinner laptop enough padding protect computer scratching rubbing bought son school past fall one thinner laptop cover soon dented scratched constant rubbing screen touchpad control left little white mark display blame computer design added padding case problem plus side case durable nothing yet ripped torn 's easy spot clean\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfN6hopzAQT-",
        "outputId": "6136badd-07ed-4c60-b093-27c98c24916b"
      },
      "source": [
        "corp_vocab = [token for token in vocab if token not in original_embeddings.keys()] # out of glove-vocab words specific to corpus\n",
        "print(corp_vocab[:10])\n",
        "print(len(corp_vocab))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['boyscout', 'm6700', 'favs', 'overstuff', 'extreemly', 'dakine', 'backpain', '3n1', '2013i', 'biggy']\n",
            "2131\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgjcloqqBoah"
      },
      "source": [
        "Removing words from vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfZeb3I4AQT_",
        "outputId": "79c1d78d-2615-449a-8555-77d3afabc8a7"
      },
      "source": [
        "known_vocab = list(vocab - set(corp_vocab))\n",
        "remove_vocab = [token for token in known_vocab if count[token]<3]\n",
        "reduced_vocab = list(set(known_vocab) - set(remove_vocab[:700])) #change 700 to desired value\n",
        "remaining_vocab = list(vocab - set(reduced_vocab))\n",
        "# test_vocab = [token for token in vocab if count[token]>2]\n",
        "print(reduced_vocab[:5])\n",
        "# small_vocab = [token for token in vocab if count[token]<3]\n",
        "print(len(known_vocab))\n",
        "print(len(reduced_vocab))\n",
        "print(len(remaining_vocab))\n",
        "# print(small_vocab[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['packrat', 'relative', 'account', 'hostile', 'blend']\n",
            "13298\n",
            "12598\n",
            "2831\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34DAVRtXAQT_",
        "outputId": "e726f261-be8c-4d0e-a186-7e53256ad9cd"
      },
      "source": [
        "t0=time()\n",
        "count_model = CountVectorizer(ngram_range=(1,5),vocabulary=reduced_vocab) # unigram to 5-gram. building only for corpus specific vocab\n",
        "X = count_model.fit_transform(corpus_doc)\n",
        "X[X > 0] = 1 # to remove within-line cooccurence\n",
        "Xc = (X.T * X) # co-occurrence matrix in sparse csr format\n",
        "Xc.setdiag(0) # setting same word cooccurence to 0\n",
        "# coocc_arr = sparse.lil_matrix(Xc).toarray()\n",
        "coocc_ar = Xc.toarray()\n",
        "t1= time()\n",
        "print(f\" cost {t1 - t0:.2f} seconds\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csc_matrix is expensive. lil_matrix is more efficient.\n",
            "  self._set_arrayXarray(i, j, x)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " cost 7.49 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RiloFoUAQUF",
        "outputId": "d5915de6-5297-4159-cf41-f7d38afa7f7f"
      },
      "source": [
        "mittens_model = Mittens(n=300, max_iter=1500)\n",
        "t0 = time()\n",
        "new_embeddings = mittens_model.fit(\n",
        "    coocc_ar,\n",
        "    vocab=reduced_vocab,\n",
        "    initial_embedding_dict= original_embeddings)\n",
        "t1=time()\n",
        "print(f\"\\ntraining cost {t1 - t0:.2f} seconds\")\n",
        "new_glove = dict(zip(reduced_vocab, new_embeddings))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/adagrad.py:77: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1500: loss: 52619.71484375"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training cost 4246.57 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLRPHK1RUN-X"
      },
      "source": [
        "Iteration 1500: loss: 52619.71484375\n",
        "training cost 4246.57 seconds\n",
        "12598 words. removed 700 words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dc_hYa2CN3z"
      },
      "source": [
        "Now co-occurence for corpus specific words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjsjPpdnB9Ij",
        "outputId": "0e290153-23e2-4484-eb30-8a1b34a46fc1"
      },
      "source": [
        "t0=time()\n",
        "count_model = CountVectorizer(ngram_range=(1,5),vocabulary=remaining_vocab) # unigram to 5-gram. building only for corpus specific vocab\n",
        "X = count_model.fit_transform(corpus_doc)\n",
        "X[X > 0] = 1 # to remove within-line cooccurence\n",
        "Xc = (X.T * X) # co-occurrence matrix in sparse csr format\n",
        "Xc.setdiag(0) # setting same word cooccurence to 0\n",
        "# coocc_arr = sparse.lil_matrix(Xc).toarray()\n",
        "coocc_ar = Xc.toarray()\n",
        "t1= time()\n",
        "print(f\" cost {t1 - t0:.2f} seconds\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " cost 6.07 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csc_matrix is expensive. lil_matrix is more efficient.\n",
            "  self._set_arrayXarray(i, j, x)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hc70dpaB46o",
        "outputId": "a71f34ba-4e64-49b7-fd2e-6e3e891054a7"
      },
      "source": [
        "mittens_model = Mittens(n=300, max_iter=2000)\n",
        "t0 = time()\n",
        "remain_embeddings = mittens_model.fit(\n",
        "    coocc_ar,\n",
        "    vocab=remaining_vocab,\n",
        "    initial_embedding_dict= original_embeddings)\n",
        "t1=time()\n",
        "print(f\"\\ntraining cost {t1 - t0:.2f} seconds\")\n",
        "# new_glove = dict(zip(remaining_vocab, remain_embeddings))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py:1761: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n",
            "Iteration 2000: loss: 0.00034659053198993206"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training cost 138.68 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGi6Tpl2AQUI"
      },
      "source": [
        "new_embeds = dict(zip(remaining_vocab, remain_embeddings))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLZemRq2AQUJ"
      },
      "source": [
        "finetuned_embeddings = {**new_glove, **new_embeds}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoACXa4jAQUK",
        "outputId": "ac6a9e51-00e8-4f8f-aea1-6cabdd439c8a"
      },
      "source": [
        "len(finetuned_embeddings)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15429"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2a63VLYAQUL"
      },
      "source": [
        " finetune_output_dir = '../wv/oposum_w2v/'\n",
        " pickle_save(finetuned_embeddings, finetune_output_dir + domain + '_glove_tuned_m3.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToT1Be8IUZjK"
      },
      "source": [
        "# Fine Tune for all Domains"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8hXs17CtKCp"
      },
      "source": [
        "Need to decide on method and then implement for all domains."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGttqqrXpzRH"
      },
      "source": [
        "def pickle_save(clean_corpus, filename):\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(clean_corpus, f)\n",
        "\n",
        "def pickle_load(filename):\n",
        "    with open(filename, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "def glove2dict(glove_filename):\n",
        "    with open(glove_filename, encoding='utf-8') as f:\n",
        "        reader = csv.reader(f, delimiter=' ', quoting=csv.QUOTE_NONE)\n",
        "        embed = {line[0]: np.array(list(map(float, line[1:])))\n",
        "                for line in reader}\n",
        "    return embed\n",
        "\n",
        "def build_vocab(corpus):\n",
        "    vocab = set()\n",
        "    count = defaultdict(int)\n",
        "    for sentence in corpus:\n",
        "      for token in sentence:\n",
        "        vocab.add(token)\n",
        "        count[token] += 1\n",
        "    return vocab, count\n",
        "\n",
        "def build_cooc_matrix(vocab, corpus_doc):\n",
        "    t0=time()\n",
        "    count_model = CountVectorizer(ngram_range=(1,5),vocabulary=vocab) # unigram to 5-gram. building only for corpus specific vocab\n",
        "    X = count_model.fit_transform(corpus_doc)\n",
        "    X[X > 0] = 1 # to remove within-line cooccurence\n",
        "    Xc = (X.T * X) # co-occurrence matrix in sparse csr format\n",
        "    Xc.setdiag(0) # setting same word cooccurence to 0\n",
        "    # coocc_arr = sparse.lil_matrix(Xc).toarray()\n",
        "    coocc_ar = Xc.toarray()\n",
        "    t1= time()\n",
        "    print(f\" Matrix build cost {t1 - t0:.2f} seconds\")\n",
        "    return coocc_ar\n",
        "\n",
        "def train_mittens(coocc_ar, vocab, original_embeddings):\n",
        "    mittens_model = Mittens(n=300, max_iter=1000)\n",
        "    t0 = time()\n",
        "    new_embeddings = mittens_model.fit(\n",
        "    coocc_ar,\n",
        "    vocab=vocab,\n",
        "    initial_embedding_dict= original_embeddings)\n",
        "    t1=time()\n",
        "    print(f\"\\nTraining cost {t1 - t0:.2f} seconds\")\n",
        "    return new_embeddings"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQHrqjNmzd-b"
      },
      "source": [
        "all_domains = ['bags_and_cases', 'bluetooth', 'boots', 'keyboards', 'tv', 'vacuums']"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGUVwxo61O2A"
      },
      "source": [
        "glove_path = \"../glove.6B/glove.6B.300d.txt\" # get it from https://nlp.stanford.edu/projects/glove\n",
        "original_embeddings = glove2dict(glove_path)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVdj9Sr_6Zi-"
      },
      "source": [
        "finetune_output_dir = '../wv/oposum_w2v/'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mn0fkC9QUjHp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89a44c3c-abc5-4538-cc10-78059878c939"
      },
      "source": [
        "for domain in all_domains:\n",
        "    t0 = time()\n",
        "    print(f\"for domain {domain}\")\n",
        "    print(f\"loading corpus for domain {domain}..\")\n",
        "    corpus_file = '../processed/oposum/' + domain + '_corpus_wotf1.pkl'\n",
        "    corpus = pickle_load(corpus_file)\n",
        "\n",
        "    print(\"building vocab\")\n",
        "    vocab, count = build_vocab(corpus)\n",
        "    corpus_doc = [' '.join(item) for item in corpus]\n",
        "    corp_vocab = [token for token in vocab if token not in original_embeddings.keys()]\n",
        "\n",
        "    random_embeds = np.random.rand(len(corp_vocab),300)\n",
        "    new_embeds = dict(zip(corp_vocab, random_embeds))\n",
        "    old_embeds = {item:original_embeddings[item] for item in vocab if item in original_embeddings}\n",
        "    pretrained_glove = {**old_embeds, **new_embeds} #combining old and new\n",
        "    print(\"saving pretrained model ...\")\n",
        "    pickle_save(pretrained_glove, finetune_output_dir + domain + '_glove_pretrained.bin')\n",
        "\n",
        "    print(\"start training ...\")\n",
        "    coocc_ar = build_cooc_matrix(corp_vocab, corpus_doc)\n",
        "    new_embeddings = train_mittens(coocc_ar, corp_vocab, original_embeddings)\n",
        "    new_embeds = dict(zip(corp_vocab, new_embeddings))\n",
        "    finetuned_glove = {**old_embeds, **new_embeds} #combining old and new\n",
        "    print(\"saving tuned model ...\")\n",
        "    pickle_save(finetuned_glove, finetune_output_dir + domain + '_glove_tuned.bin')\n",
        "\n",
        "    print(f\"finish fine-tuning on domain {domain} in {time() - t0:.2f} seconds!\\n\\n\")\n",
        "\n",
        "    # t0 = time()\n",
        "    # print(f\"for domain {domain}\")\n",
        "    # print(f\"loading corpus for domain {domain}..\")\n",
        "    # corpus_file = '../processed/oposum/' + domain + '_corpus_wotf1.pkl'\n",
        "    # corpus = pickle_load(corpus_file)\n",
        "\n",
        "    # vocab = set()\n",
        "    # for sentence in corpus:\n",
        "    #   for item in sentence:\n",
        "    #     vocab.add(item)\n",
        "      \n",
        "\n",
        "    # print(\"loading pre-trained vectors ...\")\n",
        "    # corp_vocab = [token for token in vocab if token not in original_embeddings.keys()]\n",
        "    # corpus_doc = [' '.join(item) for item in corpus]\n",
        "    # print(\"start training ...\")\n",
        "    # count_model = CountVectorizer(ngram_range=(1,5),vocabulary=corp_vocab) # unigram to 5-gram. building only for corpus specific vocab\n",
        "    # X = count_model.fit_transform(corpus_doc)\n",
        "    # X[X > 0] = 1 # to remove within-line cooccurence\n",
        "    # Xc = (X.T * X) # co-occurrence matrix in sparse csr format\n",
        "    # Xc.setdiag(0) # setting same word cooccurence to 0\n",
        "    # coocc_ar = Xc.toarray()\n",
        "\n",
        "    # t1 = time()\n",
        "    # print(f\"co-occurence matrix creation cost {t1 - t0:.2f} seconds\")\n",
        "\n",
        "    # mittens_model = Mittens(n=300, max_iter=500)\n",
        "    # new_embeddings = mittens_model.fit(\n",
        "    #     coocc_ar,\n",
        "    #     vocab=corp_vocab,\n",
        "    #     initial_embedding_dict= original_embeddings)\n",
        "    \n",
        "    # print(f\"\\ntraining cost {time() - t1:.2f} seconds\")\n",
        "\n",
        "    # print(\"save fine-tuned word vectors ...\")\n",
        "    # new_embeds = dict(zip(corp_vocab, new_embeddings))\n",
        "    # old_embeds = {item:original_embeddings[item] for item in vocab if item in original_embeddings}\n",
        "    # finetuned_embeddings = {**old_embeds, **new_embeds} #combining old and new\n",
        "    # pickle_save(finetuned_embeddings, finetune_output_dir + domain + '_glove_tuned.bin')\n",
        "\n",
        "    # print(f\"finish fine-tuning on domain {domain} in {time() - t0:.2f} seconds!\\n\\n\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "for domain bags_and_cases\n",
            "loading corpus for domain bags_and_cases..\n",
            "building vocab\n",
            "saving pretrained model ...\n",
            "start training ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csc_matrix is expensive. lil_matrix is more efficient.\n",
            "  self._set_arrayXarray(i, j, x)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Matrix build cost 6.14 seconds\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/adagrad.py:77: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 890: stopping with loss < self.tol"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training cost 62.95 seconds\n",
            "saving tuned model ...\n",
            "finish fine-tuning on domain bags_and_cases in 72.66 seconds!\n",
            "\n",
            "\n",
            "for domain bluetooth\n",
            "loading corpus for domain bluetooth..\n",
            "building vocab\n",
            "saving pretrained model ...\n",
            "start training ...\n",
            " Matrix build cost 14.39 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py:1761: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n",
            "Iteration 1000: loss: 0.0011104565346613526"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training cost 237.18 seconds\n",
            "saving tuned model ...\n",
            "finish fine-tuning on domain bluetooth in 257.49 seconds!\n",
            "\n",
            "\n",
            "for domain boots\n",
            "loading corpus for domain boots..\n",
            "building vocab\n",
            "saving pretrained model ...\n",
            "start training ...\n",
            " Matrix build cost 9.40 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 780: loss: 0.00010530331928748637"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training cost 56.87 seconds\n",
            "saving tuned model ...\n",
            "finish fine-tuning on domain boots in 69.37 seconds!\n",
            "\n",
            "\n",
            "for domain keyboards\n",
            "loading corpus for domain keyboards..\n",
            "building vocab\n",
            "saving pretrained model ...\n",
            "start training ...\n",
            " Matrix build cost 6.18 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1000: loss: 0.00014414358884096146"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training cost 99.44 seconds\n",
            "saving tuned model ...\n",
            "finish fine-tuning on domain keyboards in 108.29 seconds!\n",
            "\n",
            "\n",
            "for domain tv\n",
            "loading corpus for domain tv..\n",
            "building vocab\n",
            "saving pretrained model ...\n",
            "start training ...\n",
            " Matrix build cost 15.19 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1000: loss: 0.0034087328240275383"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training cost 442.96 seconds\n",
            "saving tuned model ...\n",
            "finish fine-tuning on domain tv in 463.10 seconds!\n",
            "\n",
            "\n",
            "for domain vacuums\n",
            "loading corpus for domain vacuums..\n",
            "building vocab\n",
            "saving pretrained model ...\n",
            "start training ...\n",
            " Matrix build cost 13.96 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1000: loss: 0.0006194550078362226"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training cost 143.59 seconds\n",
            "saving tuned model ...\n",
            "finish fine-tuning on domain vacuums in 162.39 seconds!\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7MBoxfMtrcA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}