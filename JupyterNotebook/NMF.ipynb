{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "import string\n",
    "import logging\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "                                             comment\n0                   That was another administration.\n1  Why is the ex CEO of Monsanto appointed head o...\n2  because he greased the palms of our \"elected\" ...\n3  the photo shown above is from a feed lot in Ca...\n4  THAT IS NOT GRASS FED MEET! NO GRASS THERE! YO...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>comment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>That was another administration.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Why is the ex CEO of Monsanto appointed head o...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>because he greased the palms of our \"elected\" ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>the photo shown above is from a feed lot in Ca...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>THAT IS NOT GRASS FED MEET! NO GRASS THERE! YO...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = \"../processed/comments.pkl\"\n",
    "df = pd.read_pickle(data_file)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "#df_clean = pd.DataFrame(df.comment.apply(lambda x: clean_text(x)))\n",
    "#df_clean.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "def nouns_only(text):\n",
    "    sent = []\n",
    "    text = word_tokenize(text)\n",
    "    pos_tagged = pos_tag(text)\n",
    "    noun_tags = ['NN','NNS']\n",
    "    nouns = filter(lambda x:x[1] in noun_tags,pos_tagged)\n",
    "    for word in nouns:\n",
    "        sent.append(word[0])\n",
    "    return \" \".join(sent)\n",
    "\n",
    "#df_nouns = pd.DataFrame(df_clean.comment.apply(lambda x: nouns_only(x)))\n",
    "#df_nouns.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "def lemmatizer(text):\n",
    "    sent = []\n",
    "    doc = nlp(text)\n",
    "    for word in doc:\n",
    "        sent.append(word.lemma_)\n",
    "    return \" \".join(sent)\n",
    "\n",
    "#df_clean = pd.DataFrame(df_clean.comment.apply(lambda x: lemmatizer(x)))\n",
    "#df_clean['comment'] = df_clean['comment'].str.replace('-pron-', '')\n",
    "#df_clean.head()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    sent = []\n",
    "    doc = nlp(text)\n",
    "    for word in doc:\n",
    "        if word.text not in stop_words:\n",
    "            sent.append(word.text)\n",
    "    return \" \".join(sent)\n",
    "\n",
    "#df_clean_stopwords = pd.DataFrame(df_clean.comment.apply(lambda x: remove_stopwords(x)))\n",
    "#df_clean_stopwords.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "processed_path = \"../processed\"\n",
    "def save_file(df_comment, name):\n",
    "    os.makedirs(processed_path, exist_ok=True)\n",
    "    with open(os.path.join(processed_path, name), 'wb') as f:\n",
    "        pickle.dump(df_comment, f)\n",
    "\n",
    "def open_file(name):\n",
    "    with open(os.path.join(processed_path, name), 'rb') as f:\n",
    "        df_comment = pickle.load(f)\n",
    "\n",
    "        return df_comment\n",
    "\n",
    "#df_comment = save_file(df_clean,'lemmatize_clean_comments.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "def preprocess(text, clean=True, noun=True, lemmatize=True, stopword=True):\n",
    "    if clean:\n",
    "        text = clean_text(text)\n",
    "    if noun:\n",
    "        text = nouns_only(text)\n",
    "    if stopword:\n",
    "        text = remove_stopwords(text)\n",
    "    if lemmatize:\n",
    "        text = lemmatizer(text)\n",
    "\n",
    "\n",
    "    return text\n",
    "\n",
    "#cc = 'I have a dream, I got everything I want it. When she was just a girls, she expects to the world'\n",
    "#cc1 = preprocess(cc,clean=True, noun=True, lemmatize=True, stopword=True)\n",
    "#print(cc1)\n",
    "\n",
    "#df_noun_lematize = pd.DataFrame(df.comment.apply(lambda x: preprocess(x,clean=False, noun=True, lemmatize=True, stopword=False)))\n",
    "#df_comment = save_file(df_noun_lematize,'noun_lematize_comments.pkl')\n",
    "#df_noun_lematize_stopword = pd.DataFrame(df.comment.apply(lambda x: preprocess(x,clean=False, noun=True, lemmatize=True, stopword=True)))\n",
    "#df_comment1 = save_file(df_noun_lematize_stopword,'noun_lematize_stopword_comments.pkl')\n",
    "#df_clean_lematize_stopword = pd.DataFrame(df.comment.apply(lambda x: preprocess(x,clean=True, noun=False, lemmatize=True, stopword=True)))\n",
    "#df_comment2 = save_file(df_clean_lematize_stopword,'clean_lematize_stopword_comments.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "df_comment = open_file('clean_lematize_stopword_comments.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1369)\t0.31446509274101964\n",
      "  (0, 1660)\t0.3639240316816441\n",
      "  (0, 183)\t0.5057417371243024\n",
      "  (0, 2320)\t0.25814526621515405\n",
      "  (0, 550)\t0.42892423364130633\n",
      "  (0, 1266)\t0.5121340871962907\n"
     ]
    }
   ],
   "source": [
    "#most comment words\n",
    "n_features = 4000\n",
    "\n",
    "#number of topics/aspects\n",
    "n_topics = 10\n",
    "\n",
    "#seedwords\n",
    "n_top_words = 10\n",
    "\n",
    "# ignore terms that have a document frequency strictly higher than 95%,\n",
    "# ignore terms that have a document frequency strictly lower than 2\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.99, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(df_comment['comment'].values.astype(str))\n",
    "print(tfidf[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ccche\\anaconda3\\envs\\nlp-lab\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:315: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  \"'nndsvda' in 1.1 (renaming of 0.26).\"), FutureWarning)\n",
      "C:\\Users\\ccche\\anaconda3\\envs\\nlp-lab\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1091: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n",
      "C:\\Users\\ccche\\anaconda3\\envs\\nlp-lab\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:315: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  \"'nndsvda' in 1.1 (renaming of 0.26).\"), FutureWarning)\n",
      "C:\\Users\\ccche\\anaconda3\\envs\\nlp-lab\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1091: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# alpha=0 means no regularization, l1_ratio=.5, the penalty is a combination of L1 and L2\n",
    "nmf = NMF(n_components=n_topics, random_state=1, alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "nmf_output = nmf.fit_transform(tfidf)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "data": {
      "text/plain": "          Word 0     Word 1   Word 2    Word 3       Word 4   Word 5   Word 6  \\\nTopic 0   people       know     make      like          use    think      say   \nTopic 1  organic  pesticide      use   farming      produce     mean     farm   \nTopic 2    thank      share     post      info  information    great  article   \nTopic 3      buy      local  product      stop       farmer      egg    store   \nTopic 4     food       grow    store   healthy      process   health     shop   \nTopic 5      eat       meat   animal      stop      chicken  healthy      egg   \nTopic 6      yes       vote    right        oh       course     love     hell   \nTopic 7     good      taste    thing      news         luck    point     idea   \nTopic 8     milk        cow      raw     drink        dairy   almond     baby   \nTopic 9      gmo      label     corn  monsanto          non  product     crop   \n\n               Word 7    Word 8   Word 9  \nTopic 0          pron      need     time  \nTopic 1  conventional   certify   farmer  \nTopic 2          love       god     sign  \nTopic 3        market   anymore    brand  \nTopic 4         price     label     real  \nTopic 5          diet     vegan     feed  \nTopic 6        answer  question  boycott  \nTopic 7           way       job      bad  \nTopic 8          love      calf    human  \nTopic 9          seed      free     gmos  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word 0</th>\n      <th>Word 1</th>\n      <th>Word 2</th>\n      <th>Word 3</th>\n      <th>Word 4</th>\n      <th>Word 5</th>\n      <th>Word 6</th>\n      <th>Word 7</th>\n      <th>Word 8</th>\n      <th>Word 9</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Topic 0</th>\n      <td>people</td>\n      <td>know</td>\n      <td>make</td>\n      <td>like</td>\n      <td>use</td>\n      <td>think</td>\n      <td>say</td>\n      <td>pron</td>\n      <td>need</td>\n      <td>time</td>\n    </tr>\n    <tr>\n      <th>Topic 1</th>\n      <td>organic</td>\n      <td>pesticide</td>\n      <td>use</td>\n      <td>farming</td>\n      <td>produce</td>\n      <td>mean</td>\n      <td>farm</td>\n      <td>conventional</td>\n      <td>certify</td>\n      <td>farmer</td>\n    </tr>\n    <tr>\n      <th>Topic 2</th>\n      <td>thank</td>\n      <td>share</td>\n      <td>post</td>\n      <td>info</td>\n      <td>information</td>\n      <td>great</td>\n      <td>article</td>\n      <td>love</td>\n      <td>god</td>\n      <td>sign</td>\n    </tr>\n    <tr>\n      <th>Topic 3</th>\n      <td>buy</td>\n      <td>local</td>\n      <td>product</td>\n      <td>stop</td>\n      <td>farmer</td>\n      <td>egg</td>\n      <td>store</td>\n      <td>market</td>\n      <td>anymore</td>\n      <td>brand</td>\n    </tr>\n    <tr>\n      <th>Topic 4</th>\n      <td>food</td>\n      <td>grow</td>\n      <td>store</td>\n      <td>healthy</td>\n      <td>process</td>\n      <td>health</td>\n      <td>shop</td>\n      <td>price</td>\n      <td>label</td>\n      <td>real</td>\n    </tr>\n    <tr>\n      <th>Topic 5</th>\n      <td>eat</td>\n      <td>meat</td>\n      <td>animal</td>\n      <td>stop</td>\n      <td>chicken</td>\n      <td>healthy</td>\n      <td>egg</td>\n      <td>diet</td>\n      <td>vegan</td>\n      <td>feed</td>\n    </tr>\n    <tr>\n      <th>Topic 6</th>\n      <td>yes</td>\n      <td>vote</td>\n      <td>right</td>\n      <td>oh</td>\n      <td>course</td>\n      <td>love</td>\n      <td>hell</td>\n      <td>answer</td>\n      <td>question</td>\n      <td>boycott</td>\n    </tr>\n    <tr>\n      <th>Topic 7</th>\n      <td>good</td>\n      <td>taste</td>\n      <td>thing</td>\n      <td>news</td>\n      <td>luck</td>\n      <td>point</td>\n      <td>idea</td>\n      <td>way</td>\n      <td>job</td>\n      <td>bad</td>\n    </tr>\n    <tr>\n      <th>Topic 8</th>\n      <td>milk</td>\n      <td>cow</td>\n      <td>raw</td>\n      <td>drink</td>\n      <td>dairy</td>\n      <td>almond</td>\n      <td>baby</td>\n      <td>love</td>\n      <td>calf</td>\n      <td>human</td>\n    </tr>\n    <tr>\n      <th>Topic 9</th>\n      <td>gmo</td>\n      <td>label</td>\n      <td>corn</td>\n      <td>monsanto</td>\n      <td>non</td>\n      <td>product</td>\n      <td>crop</td>\n      <td>seed</td>\n      <td>free</td>\n      <td>gmos</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def show_topics(vectorizer=tfidf_vectorizer, lda_model=nmf, n_top_words=10):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_top_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords\n",
    "\n",
    "topic_keywords = show_topics(vectorizer=tfidf_vectorizer, lda_model=nmf, n_top_words=n_top_words)\n",
    "\n",
    "# Topic - Keywords Dataframe\n",
    "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
    "\n",
    "# Topics_theme = ['Word start from ph', 'People/Friend/Relationship', 'Life/Experience/Love/Purpose', 'Money/Internet/Business',\n",
    "#                 'Weekend/Parent/Child', 'Leisure time', 'Language/technique/software', 'Relationship/Girl/Boy',\n",
    "#                 'Business relate to India, China or Pakistan', 'Friend/Love/Relationship', 'Difference and similarity/Language/Engineering',\n",
    "#                 'Culture, travel and visa requirements in several countries', 'Tips on working as software engineering', 'Book/Movie/Class/History/Physics/Chemistry/Science',\n",
    "#                 'Software engineer job opportunitis in Canada', 'Love/Life/Relationship', 'World/War/Language/History', 'Day/Hour/Week/Month/Sex/Place', 'School/Student/College/University',\n",
    "#                 'Question/Answer/Quora/Interview']\n",
    "# df_topic_keywords['topic_theme'] = Topics_theme\n",
    "# df_topic_keywords.set_index('topic_theme', inplace=True)\n",
    "# df_topic_keywords.T\n",
    "\n",
    "df_topic_keywords"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-8c6bb530",
   "language": "python",
   "display_name": "PyCharm (nlp-lab)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}