{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Oposum Corpus\r\n",
    "\r\n",
    "build corpus from oposum train/dev/test files\r\n",
    "-> tokenized, lemmatized, remove stopwords, remove tf1 items (with functions from `prep_hdf5`, i.e. the processing procedure is the same as official)\r\n",
    "\r\n",
    "update: just tokenized, lemmatized, remove stopwords"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\r\n",
    "from nltk.corpus import stopwords\r\n",
    "from tqdm import tqdm\r\n",
    "from gensim.models import Word2Vec, KeyedVectors\r\n",
    "from time import time\r\n",
    "\r\n",
    "%load_ext autoreload\r\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "%cd ../"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "c:\\Project\\group-1.3\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import oposum_scripts.prep_hdf5 as prephdf5\r\n",
    "from scripts.utils import *"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package words to C:\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "all_domains = ['bags_and_cases', 'bluetooth', 'boots', 'keyboards', 'tv', 'vacuums']\r\n",
    "train_file_folder = './LeverageJustAFewKeywords/data/train/'\r\n",
    "dev_test_file_folder = './LeverageJustAFewKeywords/data/gold/'\r\n",
    "output_corpus_basedir = './processed/oposum/'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "os.makedirs(output_corpus_basedir, exist_ok=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# debug example"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "domain = all_domains[0]\r\n",
    "train_file = train_file_folder + domain + '.trn'\r\n",
    "dev_file = dev_test_file_folder + domain + '-dev.asp'\r\n",
    "test_file = dev_test_file_folder + domain + '-tst.asp'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def from_train_file(train_file):\r\n",
    "    corpus = []\r\n",
    "    paragraph = []\r\n",
    "    with open(train_file, 'r') as f:\r\n",
    "        first_line = True\r\n",
    "        for line in tqdm(f):\r\n",
    "            if not first_line:\r\n",
    "                if len(line.strip()) != 0:\r\n",
    "                    segs, orig, ids, total = prephdf5.line_to_words(line, min_len=0, \r\n",
    "                                                stop_words=set(stopwords.words('english')))\r\n",
    "                    flatten_tokens = [t for seg in segs for t in seg]\r\n",
    "                    paragraph.extend(flatten_tokens)\r\n",
    "                else:   # a blank line\r\n",
    "                    first_line = True\r\n",
    "                    corpus.append(paragraph)\r\n",
    "                    paragraph = []\r\n",
    "            else:   # line with code\r\n",
    "                first_line = False\r\n",
    "    return corpus"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "# train_corpus = from_train_file(train_file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def print_example(data, num=5):\r\n",
    "    for i in range(num):\r\n",
    "        print(data[i])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "print(len(train_corpus))\r\n",
    "print_example(train_corpus, 5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "42632\n",
      "['case', 'look', 'nice', 'plenty', 'pocket', 'stuff', 'carry', 'around', \"'re\", 'using', 'something', 'back', 'forth', 'office', 'thinner', 'laptop', 'enough', 'padding', 'protect', 'computer', 'scratching', 'rubbing', 'bought', 'son', 'school', 'past', 'fall', 'one', 'thinner', 'laptop', 'cover', 'soon', 'dented', 'scratched', 'constant', 'rubbing', 'screen', 'touchpad', 'control', 'left', 'little', 'white', 'mark', 'display', 'blame', 'computer', 'design', 'added', 'padding', 'case', 'problem', 'plus', 'side', 'case', 'durable', 'nothing', 'yet', 'ripped', 'torn', \"'s\", 'easy', 'spot', 'clean']\n",
      "['bought', 'one', 'month', 'back', 'toshiba', 'laptop', 'case', 'quite', 'strong', 'thou', 'lill', 'small', '<NUM>', \"''\", 'toshiba', 'satellite', 'side', 'bag', 'quite', 'strong', 'sturdy', 'compact', 'u', 'r', 'thinking', 'u', 'also', 'put', 'ur', 'file', 'n', 'document', 'case', 'along', 'ur', 'laptop', 'bag', 'u', 'pro', 'strong', 'good', 'quality', 'fabric', 'n', 'zipper', 'water', 'proof', 'light', 'right', 'size', 'smart', 'looking', 'toocons', 'wont', 'recommend', '<NUM>', \"''\", 'laptop', 'wont', 'carry', 'ur', 'book', 'big', 'file', 'overall', 'think', 'good', 'bag', 'price']\n",
      "['pic', 'product', 'justice', 'real', 'thing', 'good', 'looking', 'black', 'synthetic', 'sheen', 'well', 'designed', 'isnt', 'one', 'huge', 'contraption', 'people', 'carry', 'hand', 'shoulder', 'strap', 'delicious', 'padding', 'adequate', 'space', 'large', 'computer', 'dell', 'inspiron', 'fit', 'zip', 'drive', 'cable', 'disk', 'magazine', 'note', 'etc', 'looking', 'carry', 'bulky', 'file', 'text', 'book', 'might', 'work', 'value', 'money', 'day']\n",
      "['hp', 'pavilion', 'ze5385us', 'ze5300', 'series', 'thing', 'fit', 'laptop', 'perfectly', 'front', 'pocket', 'fit', 'power', 'supply', 'infrared', 'mouse', 'infrared', 'transmitter', 'headphone', 'fine', 'another', 'large', 'pocket', 'inside', 'use', 'store', 'cdroms', 'hold', '<NUM>', 'cdrom', 'cover', 'another', 'inside', 'raised', 'pocket', 'large', 'enough', 'hold', 'box', 'business', 'card', '<NUM>', 'tablet', 'migraine', 'heache', 'bottle', 'back', 'side', 'pocket', 'slimmer', 'big', 'enough', 'hold', 'composition', 'notebook', 'newspaper', \"'s\", 'perfect', 'need', 'look', 'quite', 'decent', 'would', 'highly', 'recommend']\n",
      "[\"'m\", 'happy', 'bag', 'exceeded', 'expectation', '<NUM>', \"''\", 'toshiba', 'satellite', 'fit', 'perfectly', \"'m\", 'using', 'one', 'adjustable', 'padding', 'better', 'secure', 'notebook', 'accomodate', 'thicker', 'laptop', 'really', 'thin', 'might', 'want', 'look', 'another', 'bag', 'enough', 'compartment', 'need', 'two', 'big', 'exterior', 'one', 'folder', 'cd', \"'s\", 'interior', 'one', 'mouse', 'power', 'supply', 'great', 'value', 'sincerely', 'recommend']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def from_dev_test_file(test_file):\r\n",
    "    corpus = []\r\n",
    "    paragraph = []\r\n",
    "    with open(test_file, 'r') as f:\r\n",
    "        aspects = f.readline()\r\n",
    "        first_line = False\r\n",
    "        for line in f:\r\n",
    "            if not first_line:\r\n",
    "                if len(line.strip()) != 0:\r\n",
    "                    segment, labels = line.strip().split('\\t')\r\n",
    "                    segs, orig, ids, total = prephdf5.line_to_words(segment, min_len=0, \r\n",
    "                                        stop_words=set(stopwords.words('english')), lemmatize=True)\r\n",
    "                    flatten_segs = [t for seg in segs for t in seg]\r\n",
    "                    paragraph.extend(flatten_segs)\r\n",
    "                else:\r\n",
    "                    first_line = True\r\n",
    "                    if len(paragraph) > 0:\r\n",
    "                        corpus.append(paragraph)\r\n",
    "                    paragraph = []\r\n",
    "            else:\r\n",
    "                rcode = line.strip()\r\n",
    "                first_line = False\r\n",
    "        return corpus"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "dev_corpus = from_dev_test_file(dev_file)\r\n",
    "test_corpus = from_dev_test_file(test_file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "print(len(dev_corpus))\r\n",
    "print_example(dev_corpus, 5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "50\n",
      "['photo', 'online', 'card', 'attached', 'case', 'show', 'foam', \"n't\", 'foam', 'included', 'way', 'thin', 'thin', 'thin', 'case', 'would', 'protect', 'pencil', 'let', 'alone', 'heavy', 'laptop', \"n't\", 'waste', 'money', 'mine', 'go', 'back', 'tomorrow']\n",
      "['use', 'case', 'daily', 'basis', 'used', 'five', 'month', 'strap', 'attachment', 'handle', 'broke', 'already', 'look', 'new', 'case', 'going', 'use', 'daily', 'buy', 'something', 'better', 'quality', 'look', 'nice', 'bulky', 'brake', 'due', 'poor', 'quality']\n",
      "['really', 'liked', 'bag', 'room', 'power', 'cable', 'small', 'thing', 'year', 'used', 'occasionally', 'unfortunately', 'holiday', 'hand', 'strap', 'broke', 'great', 'buy', 'careful', 'strap']\n",
      "['bought', 'case', 'recomendation', 'friend', 'size', 'case', 'great', 'even', 'fit', 'te', 'seat', 'smaller', 'airplane', 'need', 'lot', 'space', 'extra', 'perfect', 'size', 'padding', 'good', 'quite', 'stylish', 'enough', 'size', 'mouse', 'power', 'cord', 'fre', 'cd', \"'s\", 'cell', 'phone', 'love', 'styling', 'detail', 'well', 'thought', 'although', 'one', 'drawback', 'machine', 'shy', 'maximum', 'size', 'allowed', 'th', 'spec', 'pouch', 'inside', 'computer', 'tight', 'fit', 'put', 'definite', 'drawback', 'given', 'opportunity', 'would', 'buy', 'one']\n",
      "['love', 'bag', 'glad', 'ordered', 'shopped', 'around', 'week', 'nothing', 'good', 'quality', 'bag', 'outstanding', 'design', 'perfect', 'fit', 'laptop', 'battery', 'pen', 'paperwork', 'etc', 'padding', 'inside', 'really', 'keep', 'laptop', 'secure', 'laptop', 'bag', 'saw', 'either', 'bulky', 'super', 'expensive', 'price', 'amazon', 'great', 'however', 'would', 'definately', 'paid', 'given', 'currently', 'considering', 'product', 'look', 'looking']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "print(len(test_corpus))\r\n",
    "print_example(test_corpus, 5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "50\n",
      "['purchased', 'case', 'looked', 'functional', 'surprise', 'rivet', 'popped', 'zipper', 'broke', 'two', 'week', \"'s\", 'use', 'never', 'disappointed', 'sampsonite', 'product', 'written', 'received', 'communication', 'return']\n",
      "['bought', 'case', 'dell', 'inspiron', 'e1505', '<NUM>', '<NUM>', \"''\", 'screen', 'way', 'small', 'case', 'inner', 'pocket', 'actual', 'spot', 'laptop', 'slip', 'say', 'fit', '<NUM>', \"''\", 'screen', 'ended', 'putting', 'laptop', 'outside', 'inner', 'pocket', 'barely', 'fit', 'wish', 'manufacturer', 'noticed', 'problem', 'listed', '<NUM>', \"''\", 'screen']\n",
      "['case', 'look', 'pretty', 'high', 'quality', \"'s\", 'thin', 'getting', 'laptop', 'case', 'huge', 'hassle', 'requiring', 'hand', 'lot', 'pulling', '<NUM>', '<NUM>', \"''\", 'widescreen', 'dell', 'dimension', '<NUM>', 'fit', 'snug', 'stated', 'getting', 'computer', 'pain', 'wife', 'trying', 'convince', 'return', \"'m\", 'lazy', 'think', \"'ll\", 'keep', 'also', 'wireless', 'mouse', 'bulge', 'case', 'thin', 'characteristic', 'may', 'desirable', 'case', 'low', 'profile', 'perfiferals', 'like', 'wireless', 'mouse', 'c', 'adapter', 'inch', 'thick', 'bulge', \"'s\", 'padding', 'course', 'periferals', 'probably', 'damaged', 'look', 'kinda', 'funny']\n",
      "['functionality', 'wise', 'awesome', 'room', 'lot', 'stuff', 'keep', 'computer', 'really', 'protected', \"'s\", 'important', 'feature', 'case', 'like', \"n't\", 'look', 'shiny', 'fancy', 'though', \"'s\", 'something', 'look', 'everything', 'buy', 'varies', 'person', 'another', 'obviously', 'anyway', 'recommend']\n",
      "['happy', 'samsonite', 'l35', 'notebook', 'case', 'fit', 'macbook', 'plenty', 'room', 'lot', 'pokets', 'mouse', 'mouse', 'pad', 'accessory', 'right', 'size', 'wanted', 'nice', 'looking', 'well', 'built']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "corpus = train_corpus + dev_corpus + dev_corpus\r\n",
    "print(len(corpus))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "42732\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "def remove_tf1(text_list):\r\n",
    "    '''\r\n",
    "    tokenized list of list\r\n",
    "    '''\r\n",
    "    model_tf1 = Word2Vec(min_count=1, sorted_vocab=1)\r\n",
    "    model_tf1.build_vocab(text_list)\r\n",
    "    TF1_list = []\r\n",
    "    for token, vocab in model_tf1.wv.vocab.items():\r\n",
    "        if vocab.count == 1:\r\n",
    "            TF1_list.append(token)\r\n",
    "    print(f'length of TF1 list: {len(TF1_list)}')\r\n",
    "    print(f'removing tf1 items ...')\r\n",
    "    text_list = [remove_wordlist(s, set(TF1_list)) for s in text_list]\r\n",
    "    return text_list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "corpus_wotf1 = remove_tf1(corpus)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "length of TF1 list: 14998\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def count_tokens(text_list):\r\n",
    "    n_token_corpus = 0\r\n",
    "    for text in text_list:\r\n",
    "        n_token_corpus += len(text)\r\n",
    "    return n_token_corpus"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "print(count_tokens(corpus) - count_tokens(corpus_wotf1))\r\n",
    "print(count_tokens(corpus_wotf1))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "14998\n",
      "2078103\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "print_example(corpus_wotf1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['case', 'look', 'nice', 'plenty', 'pocket', 'stuff', 'carry', 'around', \"'re\", 'using', 'something', 'back', 'forth', 'office', 'thinner', 'laptop', 'enough', 'padding', 'protect', 'computer', 'scratching', 'rubbing', 'bought', 'son', 'school', 'past', 'fall', 'one', 'thinner', 'laptop', 'cover', 'soon', 'dented', 'scratched', 'constant', 'rubbing', 'screen', 'touchpad', 'control', 'left', 'little', 'white', 'mark', 'display', 'blame', 'computer', 'design', 'added', 'padding', 'case', 'problem', 'plus', 'side', 'case', 'durable', 'nothing', 'yet', 'ripped', 'torn', \"'s\", 'easy', 'spot', 'clean']\n",
      "['bought', 'one', 'month', 'back', 'toshiba', 'laptop', 'case', 'quite', 'strong', 'thou', 'small', '<NUM>', \"''\", 'toshiba', 'satellite', 'side', 'bag', 'quite', 'strong', 'sturdy', 'compact', 'u', 'r', 'thinking', 'u', 'also', 'put', 'ur', 'file', 'n', 'document', 'case', 'along', 'ur', 'laptop', 'bag', 'u', 'pro', 'strong', 'good', 'quality', 'fabric', 'n', 'zipper', 'water', 'proof', 'light', 'right', 'size', 'smart', 'looking', 'wont', 'recommend', '<NUM>', \"''\", 'laptop', 'wont', 'carry', 'ur', 'book', 'big', 'file', 'overall', 'think', 'good', 'bag', 'price']\n",
      "['pic', 'product', 'justice', 'real', 'thing', 'good', 'looking', 'black', 'synthetic', 'sheen', 'well', 'designed', 'isnt', 'one', 'huge', 'contraption', 'people', 'carry', 'hand', 'shoulder', 'strap', 'delicious', 'padding', 'adequate', 'space', 'large', 'computer', 'dell', 'inspiron', 'fit', 'zip', 'drive', 'cable', 'disk', 'magazine', 'note', 'etc', 'looking', 'carry', 'bulky', 'file', 'text', 'book', 'might', 'work', 'value', 'money', 'day']\n",
      "['hp', 'pavilion', 'series', 'thing', 'fit', 'laptop', 'perfectly', 'front', 'pocket', 'fit', 'power', 'supply', 'infrared', 'mouse', 'infrared', 'transmitter', 'headphone', 'fine', 'another', 'large', 'pocket', 'inside', 'use', 'store', 'cdroms', 'hold', '<NUM>', 'cdrom', 'cover', 'another', 'inside', 'raised', 'pocket', 'large', 'enough', 'hold', 'box', 'business', 'card', '<NUM>', 'tablet', 'migraine', 'bottle', 'back', 'side', 'pocket', 'slimmer', 'big', 'enough', 'hold', 'composition', 'notebook', 'newspaper', \"'s\", 'perfect', 'need', 'look', 'quite', 'decent', 'would', 'highly', 'recommend']\n",
      "[\"'m\", 'happy', 'bag', 'exceeded', 'expectation', '<NUM>', \"''\", 'toshiba', 'satellite', 'fit', 'perfectly', \"'m\", 'using', 'one', 'adjustable', 'padding', 'better', 'secure', 'notebook', 'accomodate', 'thicker', 'laptop', 'really', 'thin', 'might', 'want', 'look', 'another', 'bag', 'enough', 'compartment', 'need', 'two', 'big', 'exterior', 'one', 'folder', 'cd', \"'s\", 'interior', 'one', 'mouse', 'power', 'supply', 'great', 'value', 'sincerely', 'recommend']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "wotf1 = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "for domain in all_domains:\r\n",
    "    print(f'processing oposum-{domain} corpus')\r\n",
    "    t0 = time()\r\n",
    "    train_file = train_file_folder + domain + '.trn'\r\n",
    "    dev_file = dev_test_file_folder + domain + '-dev.asp'\r\n",
    "    test_file = dev_test_file_folder + domain + '-tst.asp'\r\n",
    "\r\n",
    "    print(\"loading files ...\")\r\n",
    "    train_corpus = from_train_file(train_file)\r\n",
    "    dev_corpus = from_dev_test_file(dev_file)\r\n",
    "    test_corpus = from_dev_test_file(test_file)\r\n",
    "    corpus = train_corpus + dev_corpus + test_corpus\r\n",
    "    print(f\"data length of {domain}: \\ntrain: {len(train_corpus)}\\tdev: {len(dev_corpus)}\\ttest: {len(test_corpus)}\")\r\n",
    "    print(f\"number of tokens: {count_tokens(corpus)}\")\r\n",
    "    unique_tokens = set([t for p in corpus for t in p])\r\n",
    "    print(f\"number of unique tokens: {len(unique_tokens)}\")\r\n",
    "    \r\n",
    "    if wotf1:\r\n",
    "        corpus = remove_tf1(corpus)\r\n",
    "        pickle_save(corpus, output_corpus_basedir + domain + '_corpus_wotf1.pkl')\r\n",
    "    else:\r\n",
    "        pickle_save(corpus, output_corpus_basedir + domain + '_corpus.pkl')\r\n",
    "    print(f\"finished {domain} corpus processing in {time() - t0:.2f} seconds!\\n\\n\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "processing oposum-bags_and_cases corpus\n",
      "loading files ...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "338439it [02:08, 2624.47it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "data length of bags_and_cases: \n",
      "train: 42632\tdev: 50 \t test: 50\n",
      "number of tokens: 2093265\n",
      "number of unique tokens: 30430\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "241it [00:00, 2387.49it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "finished bags_and_cases corpus processing in 131.24 seconds!\n",
      "\n",
      "\n",
      "processing oposum-bluetooth corpus\n",
      "loading files ...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "758360it [04:44, 2667.56it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "data length of bluetooth: \n",
      "train: 80148\tdev: 50 \t test: 50\n",
      "number of tokens: 4820127\n",
      "number of unique tokens: 51248\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "245it [00:00, 2427.18it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "finished bluetooth corpus processing in 288.55 seconds!\n",
      "\n",
      "\n",
      "processing oposum-boots corpus\n",
      "loading files ...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "579937it [03:13, 3001.71it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "data length of boots: \n",
      "train: 77493\tdev: 50 \t test: 50\n",
      "number of tokens: 3090152\n",
      "number of unique tokens: 30345\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "235it [00:00, 2328.09it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "finished boots corpus processing in 196.40 seconds!\n",
      "\n",
      "\n",
      "processing oposum-keyboards corpus\n",
      "loading files ...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "319222it [02:02, 2616.15it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "data length of keyboards: \n",
      "train: 33613\tdev: 50 \t test: 50\n",
      "number of tokens: 2135486\n",
      "number of unique tokens: 34081\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "finished keyboards corpus processing in 124.28 seconds!\n",
      "\n",
      "\n",
      "processing oposum-tv corpus\n",
      "loading files ...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "714327it [04:40, 2549.78it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "data length of tv: \n",
      "train: 56410\tdev: 50 \t test: 50\n",
      "number of tokens: 5204820\n",
      "number of unique tokens: 59077\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "251it [00:00, 2486.20it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "finished tv corpus processing in 284.56 seconds!\n",
      "\n",
      "\n",
      "processing oposum-vacuums corpus\n",
      "loading files ...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "751318it [04:37, 2709.01it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "data length of vacuums: \n",
      "train: 68166\tdev: 50 \t test: 50\n",
      "number of tokens: 5007647\n",
      "number of unique tokens: 46243\n",
      "finished vacuums corpus processing in 281.79 seconds!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# MISC"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "lemmatizer = WordNetLemmatizer()\r\n",
    "lemmatizer.lemmatize('liked')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'liked'"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d7be162faa9619c70b8a448037d12fbee0f966914265961e1eb040c7257eee03"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('nlp': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}