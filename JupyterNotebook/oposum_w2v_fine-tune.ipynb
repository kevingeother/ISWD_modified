{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# fine-tuning w2v on oposum\r\n",
    "\r\n",
    "for each domain: build vocab on corpus (train/dev/test), load pre-trained embedding from google-news, save the intersected w2v model, after training, save the fine-tuned version of w2v model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "import gensim\r\n",
    "import logging\r\n",
    "from gensim.models import Word2Vec, KeyedVectors\r\n",
    "import os\r\n",
    "from time import time\r\n",
    "\r\n",
    "%load_ext autoreload\r\n",
    "%autoreload 2\r\n",
    "\r\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "%cd ../"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "c:\\Project\\group-1.3\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from scripts.utils import *"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package words to C:\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# example setup"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "domain = 'bags_and_cases'\r\n",
    "corpus_file = './processed/oposum/' + domain + '_corpus.pkl'\r\n",
    "corpus = pickle_load(corpus_file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "model = Word2Vec(size=300)\r\n",
    "model.build_vocab(corpus, min_count=5)\r\n",
    "total_examples = model.corpus_count\r\n",
    "print(model.wv.vectors.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(9040, 300)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "vocab size: \r\n",
    "- (15429, 300), corpus_wostw_wotf1, min_count=1\r\n",
    "- (9040, 300), wostw_wotf1, min_count=5\r\n",
    "- (30430, 300), wostw, min_count=1\r\n",
    "- (9040, 300)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.intersect_word2vec_format('./wv/GoogleNews-vectors-negative300.bin.gz', binary=True, lockf=1.0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.wv.save_word2vec_format(\"./wv/oposum/\" + domain + '_pretrained.bin', binary=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.train(corpus, total_examples=total_examples, epochs=100)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.wv.save_word2vec_format(\"./wv/oposum/\" + domain + '_tuned.bin', binary=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# fine-tuning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "all_domains = ['bags_and_cases', 'bluetooth', 'boots', 'keyboards', 'tv', 'vacuums']\r\n",
    "pretrained_w2v_file = './wv/GoogleNews-vectors-negative300.bin.gz'\r\n",
    "finetune_output_dir = './wv/oposum/'\r\n",
    "eps = 100"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "os.makedirs(finetune_output_dir, exist_ok=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "for domain in all_domains:\r\n",
    "    t0 = time()\r\n",
    "    print(f\"for domain {domain}\")\r\n",
    "    corpus_file = './processed/oposum/' + domain + '_corpus.pkl'\r\n",
    "    corpus = pickle_load(corpus_file)\r\n",
    "\r\n",
    "    model = Word2Vec(size=300)\r\n",
    "    model.build_vocab(corpus, min_count=1)\r\n",
    "    total_examples = model.corpus_count\r\n",
    "    print(f\"vocab size: {model.wv.vectors.shape[0]}\")\r\n",
    "\r\n",
    "    print(\"loading pre-trained vectors ...\")\r\n",
    "    model.intersect_word2vec_format(pretrained_w2v_file, binary=True, lockf=1.0)\r\n",
    "    print(\"save intersected pre-trained word vectors ...\")\r\n",
    "    model.wv.save_word2vec_format(finetune_output_dir + domain + '_pretrained.bin', binary=True)\r\n",
    "    print(\"start training ...\")\r\n",
    "    t1 = time()\r\n",
    "    model.train(corpus, total_examples=total_examples, epochs=eps)\r\n",
    "    print(f\"training cost {time() - t1:.2f} seconds\")\r\n",
    "    print(\"save fine-tuned word vectors ...\")\r\n",
    "    model.wv.save_word2vec_format(finetune_output_dir + domain + '_tuned.bin', binary=True)\r\n",
    "    print(f\"finish fine-tuning on domain {domain} in {time() - t0:.2f} seconds!\\n\\n\")\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "for domain bags_and_cases\n",
      "vocab size: 30430\n",
      "loading pre-trained vectors ...\n",
      "save intersected pre-trained word vectors ...\n",
      "start training ...\n",
      "training cost 604.11 seconds\n",
      "save fine-tuned word vectors ...\n",
      "finish fine-tuning on domain bags_and_cases in 742.97 seconds!\n",
      "\n",
      "\n",
      "for domain bluetooth\n",
      "vocab size: 51248\n",
      "loading pre-trained vectors ...\n",
      "save intersected pre-trained word vectors ...\n",
      "start training ...\n",
      "training cost 1342.57 seconds\n",
      "save fine-tuned word vectors ...\n",
      "finish fine-tuning on domain bluetooth in 1529.46 seconds!\n",
      "\n",
      "\n",
      "for domain boots\n",
      "vocab size: 30345\n",
      "loading pre-trained vectors ...\n",
      "save intersected pre-trained word vectors ...\n",
      "start training ...\n",
      "training cost 781.91 seconds\n",
      "save fine-tuned word vectors ...\n",
      "finish fine-tuning on domain boots in 907.57 seconds!\n",
      "\n",
      "\n",
      "for domain keyboards\n",
      "vocab size: 34081\n",
      "loading pre-trained vectors ...\n",
      "save intersected pre-trained word vectors ...\n",
      "start training ...\n",
      "training cost 582.01 seconds\n",
      "save fine-tuned word vectors ...\n",
      "finish fine-tuning on domain keyboards in 709.09 seconds!\n",
      "\n",
      "\n",
      "for domain tv\n",
      "vocab size: 59077\n",
      "loading pre-trained vectors ...\n",
      "save intersected pre-trained word vectors ...\n",
      "start training ...\n",
      "training cost 1451.04 seconds\n",
      "save fine-tuned word vectors ...\n",
      "finish fine-tuning on domain tv in 1578.06 seconds!\n",
      "\n",
      "\n",
      "for domain vacuums\n",
      "vocab size: 46243\n",
      "loading pre-trained vectors ...\n",
      "save intersected pre-trained word vectors ...\n",
      "start training ...\n",
      "training cost 1389.34 seconds\n",
      "save fine-tuned word vectors ...\n",
      "finish fine-tuning on domain vacuums in 1519.02 seconds!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d7be162faa9619c70b8a448037d12fbee0f966914265961e1eb040c7257eee03"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('nlp': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}