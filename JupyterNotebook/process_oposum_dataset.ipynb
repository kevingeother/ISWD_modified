{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "process_oposum_dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcFFfzdeNpnM"
      },
      "source": [
        "# oposum dataset processing\n",
        "\n",
        "- dealing with vocab, which is loaded from **word2vec** model or **Glove** model\n",
        "- transform sentence string to index using vocab, save for easy accessibility, `data_idx`\n",
        "- also the length of corresponding sentence, `data_length`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c956AjdUuF1j",
        "outputId": "c94648f0-4ef9-4849-9f64-f65cff512288"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# %cd /content/drive/MyDrive/group-1.3-master/group-1.3-master/LeverageJustAFewKeywords/\n",
        "# %cd /content/drive/MyDrive/LeverageJustAFewKeywords/\n",
        "%cd /content/drive/MyDrive/group-1.3/LeverageJustAFewKeywords/\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/.shortcut-targets-by-id/1n0oSoMBR4TlxDwAce51xBgon3LxJjCkE/group-1.3/LeverageJustAFewKeywords\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-07-13T15:44:29.448936Z",
          "start_time": "2021-07-13T15:44:27.389110Z"
        },
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVbDc_3XNpnc",
        "outputId": "da6f3310-3777-4010-db97-4b2459674e37"
      },
      "source": [
        "import nltk\n",
        "nltk.download('words')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "import os\n",
        "import json\n",
        "import gensim\n",
        "from utils import *\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pickle"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bL9y3TTANpng"
      },
      "source": [
        "# parameter setting and function definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqQlzCCm1OuM"
      },
      "source": [
        "def pickle_load(filename):\n",
        "    with open(filename, 'rb') as f:\n",
        "        return pickle.load(f)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-07-13T15:44:29.458917Z",
          "start_time": "2021-07-13T15:44:29.453918Z"
        },
        "id": "0FsFTCTkNpnh"
      },
      "source": [
        "oposum_domains = ['bags_and_cases', 'bluetooth', 'boots', 'keyboards', 'tv', 'vacuums']\n",
        "domain = oposum_domains[0]\n",
        "dataset_mode = 'train'\n",
        "w2v_folder = '../wv/oposum_w2v/'\n",
        "wv_mode =  'pretrained' #'tuned'\n",
        "data_folder = './data/'\n",
        "processed_folder = './data/'\n",
        "pretrained = 'glove' # 'word2vec'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-07-13T15:44:31.245949Z",
          "start_time": "2021-07-13T15:44:31.238952Z"
        },
        "id": "jvL4eOPJNpni"
      },
      "source": [
        "def load_data(file):\n",
        "    '''we use our own vocab, loading original maybe enough'''\n",
        "    with open(file) as f:\n",
        "        data = json.load(f)\n",
        "    data = [s for d in data['original'] for s in d]\n",
        "    return data\n",
        "\n",
        "def build_shift_vocab_word2vec(model_file, num_tag=2):\n",
        "    emb = gensim.models.KeyedVectors.load_word2vec_format(model_file, binary=True)\n",
        "    shift = num_tag\n",
        "    vocab = {token: i + shift for i, token in enumerate(emb.wv.index2word)}\n",
        "    vocab['<PAD>'] = 0\n",
        "    vocab['<UNK>'] = 1\n",
        "    return vocab\n",
        "\n",
        "def build_shift_vocab_glove(glove_filename, num_tag=2):\n",
        "    shift = num_tag\n",
        "    with open(glove_filename, 'rb') as f:\n",
        "      glove = pickle.load(f)\n",
        "    vocab = {token: i + shift for i, token in enumerate(glove.keys())}\n",
        "    vocab['<PAD>'] = 0\n",
        "    vocab['<UNK>'] = 1\n",
        "    return vocab\n",
        "\n",
        "def build_text_index(sentence, vocab):\n",
        "    '''transform sentence string to index according to vocab'''\n",
        "    # senc = tokenize_sentence(sentence)\n",
        "    senc = sentence.split() # oposum dataset don't need tokenizer, just .split()\n",
        "    senc = remove_wordlist(senc, set(stopwords.words('english')))\n",
        "    senc = lemmatize_sentence(senc)\n",
        "    idx = [vocab.get(token, vocab['<UNK>']) for token in senc]  # not existing token is <UNK>\n",
        "    return idx\n",
        "\n",
        "def write_vocab(vocab, file_name):\n",
        "    with open(file_name, 'w') as f:\n",
        "        for token, idx in vocab.items():\n",
        "            f.write(f\"{token}\\t{idx}\\n\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JItn5zYcNpnl"
      },
      "source": [
        "# example for debug"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-07-12T22:40:28.677300Z",
          "start_time": "2021-07-12T22:40:28.673303Z"
        },
        "id": "UpMDlNMyNpnm"
      },
      "source": [
        "data_file = os.path.join(data_folder, f'{domain}_train.json')\n",
        "data_orig = load_data(data_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-07-12T22:59:42.750713Z",
          "start_time": "2021-07-12T22:59:41.823233Z"
        },
        "scrolled": false,
        "id": "T0sxpGcYNpnn"
      },
      "source": [
        "if pretrained == 'word2vec':\n",
        "  model_file = os.path.join(w2v_folder, f\"{domain}_{wv_mode}.bin\")\n",
        "  vocab = build_shift_vocab_word2vec(model_file)\n",
        "elif pretrained == 'glove':\n",
        "  model_file = os.path.join(w2v_folder, f\"{domain}_glove_{wv_mode}.bin\")\n",
        "  vocab = build_shift_vocab_glove(model_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-07-12T22:50:15.993155Z",
          "start_time": "2021-07-12T22:46:45.852638Z"
        },
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1C0usGxNpns",
        "outputId": "8d9a0194-335c-4d4d-a09d-a04ad8a7f5a8"
      },
      "source": [
        "data_idx = []\n",
        "for s in tqdm(data_orig):\n",
        "    data_idx.append(build_text_index(s, vocab))\n",
        "# data_idx = [build_text_index(s, vocab) for s in data_orig]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 588229/588229 [01:52<00:00, 5250.66it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-07-12T22:50:29.929129Z",
          "start_time": "2021-07-12T22:50:29.843179Z"
        },
        "id": "5F6avDtbNpnt"
      },
      "source": [
        "data_length = [len(s) for s in data_idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-07-12T23:00:17.767503Z",
          "start_time": "2021-07-12T23:00:17.762506Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-hwwhy0Npnu",
        "outputId": "c94310d7-5003-4bf7-970e-b9b34d33aac8"
      },
      "source": [
        "# data_orig\n",
        "vocab['<PAD>']\n",
        "# print(vocab)\n",
        "# np.max(data_length)\n",
        "# vocab['happy']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12184"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-07-12T23:05:44.093987Z",
          "start_time": "2021-07-12T23:05:44.055006Z"
        },
        "id": "lF2psC8DNpnv"
      },
      "source": [
        "if pretrained == 'word2vec':\n",
        "  vocab_file = os.path.join(data_folder, f'{domain}_vocab_w2v.txt')\n",
        "elif pretrained == 'glove':\n",
        "  vocab_file = os.path.join(data_folder, f'{domain}_vocab_glove.txt')\n",
        "write_vocab(vocab, vocab_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-07-12T23:11:53.602020Z",
          "start_time": "2021-07-12T23:11:53.598022Z"
        },
        "id": "z56vXRxINpnv"
      },
      "source": [
        "supplement_data = {'data_idx': data_idx, 'data_length': data_length}\n",
        "if pretrained == 'word2vec':\n",
        "  supplement_data_path = os.path.join(data_folder, f'{domain}_{dataset_mode}_supplement_w2v.pkl')\n",
        "elif pretrained == 'glove':\n",
        "  supplement_data_path = os.path.join(data_folder, f'{domain}_{dataset_mode}_supplement_glove.pkl')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-07-12T23:12:32.833411Z",
          "start_time": "2021-07-12T23:12:32.504617Z"
        },
        "id": "gmvqIbdANpnw"
      },
      "source": [
        "with open(supplement_data_path, 'wb') as f:\n",
        "    pickle.dump(supplement_data, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhtE1--eNpnx"
      },
      "source": [
        "# group together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-07-13T16:18:55.028816Z",
          "start_time": "2021-07-13T15:44:39.302305Z"
        },
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5r8AcUeNpny",
        "outputId": "51b2000f-7d34-4d82-94c5-27ecf9bfdfcd"
      },
      "source": [
        "for domain in oposum_domains:\n",
        "    print(f'process {domain} ...\\n')\n",
        "    print(f'type {pretrained}...\\n')\n",
        "    data_file = os.path.join(data_folder, f'{domain}_{dataset_mode}.json')\n",
        "    data_orig = load_data(data_file)\n",
        "    if pretrained == 'word2vec':\n",
        "      model_file = os.path.join(w2v_folder, f\"{domain}_{wv_mode}.bin\")\n",
        "      vocab = build_shift_vocab_word2vec(model_file)\n",
        "    elif pretrained == 'glove':\n",
        "      model_file = os.path.join(w2v_folder, f\"{domain}_glove_{wv_mode}.bin\")\n",
        "      vocab = build_shift_vocab_glove(model_file)\n",
        "    print(f'data file: {data_file}\\nmodel file: {model_file}')\n",
        "    # model_file = os.path.join(w2v_folder, f\"{domain}_{wv_mode}.bin\")\n",
        "    # vocab = build_shift_vocab_word2vec(model_file)\n",
        "    print(f'vocab length: {len(vocab)}')\n",
        "    data_idx = []\n",
        "    print('transforming to index ...')\n",
        "    for s in tqdm(data_orig):\n",
        "        data_idx.append(build_text_index(s, vocab))\n",
        "    data_length = [len(s) for s in data_idx]\n",
        "    \n",
        "    # vocab_file = os.path.join(data_folder, f'{domain}_vocab_w2v.txt')\n",
        "    if pretrained == 'word2vec':\n",
        "      vocab_file = os.path.join(data_folder, f'{domain}_vocab_w2v.txt')\n",
        "    elif pretrained == 'glove':\n",
        "      vocab_file = os.path.join(data_folder, f'{domain}_vocab_glove.txt')\n",
        "\n",
        "    if dataset_mode == 'train':\n",
        "        write_vocab(vocab, vocab_file)\n",
        "        print('finish writing vocab file')\n",
        "\n",
        "    # supplement_data = {'data_idx': data_idx,\n",
        "    #                    'data_length': data_length}\n",
        "    # supplement_data_path = os.path.join(data_folder, f'{domain}_{dataset_mode}_supplement_w2v.pkl')\n",
        "    supplement_data = {'data_idx': data_idx, 'data_length': data_length}\n",
        "    if pretrained == 'word2vec':\n",
        "      supplement_data_path = os.path.join(data_folder, f'{domain}_{dataset_mode}_supplement_w2v.pkl')\n",
        "    elif pretrained == 'glove':\n",
        "      supplement_data_path = os.path.join(data_folder, f'{domain}_{dataset_mode}_supplement_glove.pkl')\n",
        "    with open(supplement_data_path, 'wb') as f:\n",
        "        pickle.dump(supplement_data, f)\n",
        "    print(f'finish processing {domain}\\n\\n')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "process bags_and_cases ...\n",
            "\n",
            "type glove pretrained...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/588229 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "data file: ./data/bags_and_cases_train.json\n",
            "model file: ../wv/oposum_w2v/bags_and_cases_glove_pretrained.bin\n",
            "vocab length: 15431\n",
            "transforming to index ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 588229/588229 [01:53<00:00, 5167.96it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "finish writing vocab file\n",
            "finish processing bags_and_cases\n",
            "\n",
            "\n",
            "process bluetooth ...\n",
            "\n",
            "type glove pretrained...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 534/1431839 [00:00<04:28, 5339.22it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "data file: ./data/bluetooth_train.json\n",
            "model file: ../wv/oposum_w2v/bluetooth_glove_pretrained.bin\n",
            "vocab length: 23616\n",
            "transforming to index ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1431839/1431839 [04:27<00:00, 5356.43it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "finish writing vocab file\n",
            "finish processing bluetooth\n",
            "\n",
            "\n",
            "process boots ...\n",
            "\n",
            "type glove pretrained...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 623/963866 [00:00<02:34, 6226.19it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "data file: ./data/boots_train.json\n",
            "model file: ../wv/oposum_w2v/boots_glove_pretrained.bin\n",
            "vocab length: 16088\n",
            "transforming to index ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 963866/963866 [02:59<00:00, 5377.11it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "finish writing vocab file\n",
            "finish processing boots\n",
            "\n",
            "\n",
            "process keyboards ...\n",
            "\n",
            "type glove pretrained...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 624/608801 [00:00<01:37, 6239.65it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "data file: ./data/keyboards_train.json\n",
            "model file: ../wv/oposum_w2v/keyboards_glove_pretrained.bin\n",
            "vocab length: 16542\n",
            "transforming to index ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 608801/608801 [01:54<00:00, 5313.96it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "finish writing vocab file\n",
            "finish processing keyboards\n",
            "\n",
            "\n",
            "process tv ...\n",
            "\n",
            "type glove pretrained...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 612/1432384 [00:00<03:54, 6114.09it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "data file: ./data/tv_train.json\n",
            "model file: ../wv/oposum_w2v/tv_glove_pretrained.bin\n",
            "vocab length: 27449\n",
            "transforming to index ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1432384/1432384 [04:31<00:00, 5281.65it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "finish writing vocab file\n",
            "finish processing tv\n",
            "\n",
            "\n",
            "process vacuums ...\n",
            "\n",
            "type glove pretrained...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 450/1465525 [00:00<05:26, 4492.25it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "data file: ./data/vacuums_train.json\n",
            "model file: ../wv/oposum_w2v/vacuums_glove_pretrained.bin\n",
            "vocab length: 22645\n",
            "transforming to index ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1465525/1465525 [04:33<00:00, 5353.16it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "finish writing vocab file\n",
            "finish processing vacuums\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-07-13T16:19:08.334268Z",
          "start_time": "2021-07-13T16:19:08.331270Z"
        },
        "id": "5SjBE8hFNpnz"
      },
      "source": [
        "dataset_mode = 'test'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-07-13T16:19:16.786399Z",
          "start_time": "2021-07-13T16:19:08.842974Z"
        },
        "id": "Boex2oxuNpnz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b08d0e8-27aa-4cc6-eb2a-af9d94a5bc51"
      },
      "source": [
        "for domain in oposum_domains:\n",
        "    print(f'process {domain} ...\\n')\n",
        "    print(f'pretrained {pretrained} ...\\n')\n",
        "    data_file = os.path.join(data_folder, f'{domain}_{dataset_mode}.json')\n",
        "    data_orig = load_data(data_file)\n",
        "    if pretrained == 'word2vec':\n",
        "      model_file = os.path.join(w2v_folder, f\"{domain}_{wv_mode}.bin\")\n",
        "      vocab = build_shift_vocab_word2vec(model_file)\n",
        "    elif pretrained == 'glove':\n",
        "      model_file = os.path.join(w2v_folder, f\"{domain}_glove_{wv_mode}.bin\")\n",
        "      vocab = build_shift_vocab_glove(model_file)\n",
        "    print(f'data file: {data_file}\\nmodel file: {model_file}')\n",
        "    # model_file = os.path.join(w2v_folder, f\"{domain}_{wv_mode}.bin\")\n",
        "    # vocab = build_shift_vocab_word2vec(model_file)\n",
        "    print(f'vocab length: {len(vocab)}')\n",
        "    data_idx = []\n",
        "    print('transforming to index ...')\n",
        "    for s in tqdm(data_orig):\n",
        "        data_idx.append(build_text_index(s, vocab))\n",
        "    data_length = [len(s) for s in data_idx]\n",
        "    \n",
        "    # vocab_file = os.path.join(data_folder, f'{domain}_vocab_w2v.txt')\n",
        "    if pretrained == 'word2vec':\n",
        "      vocab_file = os.path.join(data_folder, f'{domain}_vocab_w2v.txt')\n",
        "    elif pretrained == 'glove':\n",
        "      vocab_file = os.path.join(data_folder, f'{domain}_vocab_glove.txt')\n",
        "\n",
        "    if dataset_mode == 'train':\n",
        "        write_vocab(vocab, vocab_file)\n",
        "        print('finish writing vocab file')\n",
        "\n",
        "    # supplement_data = {'data_idx': data_idx,\n",
        "    #                    'data_length': data_length}\n",
        "    # supplement_data_path = os.path.join(data_folder, f'{domain}_{dataset_mode}_supplement_w2v.pkl')\n",
        "    supplement_data = {'data_idx': data_idx, 'data_length': data_length}\n",
        "    if pretrained == 'word2vec':\n",
        "      supplement_data_path = os.path.join(data_folder, f'{domain}_{dataset_mode}_supplement_w2v.pkl')\n",
        "    elif pretrained == 'glove':\n",
        "      supplement_data_path = os.path.join(data_folder, f'{domain}_{dataset_mode}_supplement_glove.pkl')\n",
        "    with open(supplement_data_path, 'wb') as f:\n",
        "        pickle.dump(supplement_data, f)\n",
        "    print(f'finish processing {domain}\\n\\n')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "process bags_and_cases ...\n",
            "\n",
            "pretrained glove ...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 653/653 [00:00<00:00, 4636.39it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "data file: ./data/bags_and_cases_test.json\n",
            "model file: ../wv/oposum_w2v/bags_and_cases_glove_pretrained.bin\n",
            "vocab length: 15431\n",
            "transforming to index ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "finish processing bags_and_cases\n",
            "\n",
            "\n",
            "process bluetooth ...\n",
            "\n",
            "pretrained glove ...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 667/667 [00:00<00:00, 4939.77it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "data file: ./data/bluetooth_test.json\n",
            "model file: ../wv/oposum_w2v/bluetooth_glove_pretrained.bin\n",
            "vocab length: 23616\n",
            "transforming to index ...\n",
            "finish processing bluetooth\n",
            "\n",
            "\n",
            "process boots ...\n",
            "\n",
            "pretrained glove ...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 631/631 [00:00<00:00, 5435.00it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "data file: ./data/boots_test.json\n",
            "model file: ../wv/oposum_w2v/boots_glove_pretrained.bin\n",
            "vocab length: 16088\n",
            "transforming to index ...\n",
            "finish processing boots\n",
            "\n",
            "\n",
            "process keyboards ...\n",
            "\n",
            "pretrained glove ...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 698/698 [00:00<00:00, 4973.29it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "data file: ./data/keyboards_test.json\n",
            "model file: ../wv/oposum_w2v/keyboards_glove_pretrained.bin\n",
            "vocab length: 16542\n",
            "transforming to index ...\n",
            "finish processing keyboards\n",
            "\n",
            "\n",
            "process tv ...\n",
            "\n",
            "pretrained glove ...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 764/764 [00:00<00:00, 5411.54it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "data file: ./data/tv_test.json\n",
            "model file: ../wv/oposum_w2v/tv_glove_pretrained.bin\n",
            "vocab length: 27449\n",
            "transforming to index ...\n",
            "finish processing tv\n",
            "\n",
            "\n",
            "process vacuums ...\n",
            "\n",
            "pretrained glove ...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 741/741 [00:00<00:00, 5227.84it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "data file: ./data/vacuums_test.json\n",
            "model file: ../wv/oposum_w2v/vacuums_glove_pretrained.bin\n",
            "vocab length: 22645\n",
            "transforming to index ...\n",
            "finish processing vacuums\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWgrt58LNpn1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}